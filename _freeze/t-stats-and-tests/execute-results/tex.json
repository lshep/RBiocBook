{
  "hash": "4c9267cee20d1655bef8f34d3a113fcd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The t-statistic and t-distribution\"\n---\n\n## Background\n\nThe t-test is a [statistical hypothesis test](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing) that is \ncommonly used when the data are normally distributed (follow a normal \ndistribution) if the value of the population standard deviation were known. When\nthe population standard deviation is not known and is replaced by an estimate\nbased no the data, the test statistic follows a Student's t distribution. \n\nT-tests are \nhandy hypothesis tests in statistics when you want to compare means. You can \ncompare a sample mean to a hypothesized or target value using a one-sample \nt-test. You can compare the means of two groups with a two-sample t-test. If \nyou have two groups with paired observations (e.g., before and after \nmeasurements), use the paired t-test.\n\nA t-test looks at the t-statistic, the t-distribution values, and \nthe degrees of freedom to determine the statistical significance. \nTo conduct a test with three or more means, we would use an analysis \nof variance.\n\nThe distriubution that the t-statistic follows was described in a famous \npaper [@student_probable_1908] by \"Student\", a pseudonym for [William Sealy Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset). \n\n## The Z-score and probability\n\nBefore talking about the t-distribution and t-scores, lets review the Z-score, \nits relation to the normal distribution, and probability.\n\nThe Z-score is defined as:\n\n$$Z = \\frac{x - \\mu}{\\sigma}$$ {#eq-zscore}\n\nwhere $\\mu$ is a the population mean from which $x$ is drawn and $\\sigma$ is\nthe population standard deviation (taken as known, not estimated from the data).\n\nThe probability of observing a $Z$ score of $z$ or greater can be calculated by\n$pnorm(z,\\mu,\\sigma)$.\n\nFor example, let's assume that our \"population\" is known and it truly\nhas a mean 0 and standard deviation 1. If we have observations drawn from\nthat population, we can assign a probability of seeing that observation\nby random chance _under the assumption that the null hypothesis is **TRUE**_. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nzscore = seq(-5,5,1)\n```\n:::\n\n\nFor each value of zscore, let's calculate the p-value and put the results in a `data.frame`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = data.frame(\n    zscore = zscore,\n    pval   = pnorm(zscore, 0, 1)\n)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   zscore         pval\n1      -5 2.866516e-07\n2      -4 3.167124e-05\n3      -3 1.349898e-03\n4      -2 2.275013e-02\n5      -1 1.586553e-01\n6       0 5.000000e-01\n7       1 8.413447e-01\n8       2 9.772499e-01\n9       3 9.986501e-01\n10      4 9.999683e-01\n11      5 9.999997e-01\n```\n\n\n:::\n:::\n\n\nWhy is the p-value of something 5 population standard deviations away from\nthe mean (zscore=5) nearly 1 in this calculation? What is the default for\n`pnorm` with respect to being one-sided or two-sided?\n\nLet's plot the values of probability vs z-score:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(df$zscore, df$pval, type='b')\n```\n\n::: {.cell-output-display}\n![](t-stats-and-tests_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThis plot is the *empirical* cumulative density function (cdf) for our data.\nHow can we use it? If we know the z-score, we can look up the probability of\nobserving that value. Since we have constructed our experiment to follow the\nstandard normal distribution, this cdf also represents the cdf of the standard\nnormal distribution.\n\n### Small diversion: two-sided pnorm function\n\nThe `pnorm` function returns the \"one-sided\" probability of having\na value at least as extreme as the observed $x$ and uses the \"lower\" tail\nby default. Let's create a function that computes two-sided p-values.\n\n1. Take the absolute value of x\n2. Compute `pnorm` with `lower.tail=FALSE` so we get lower p-values with\nlarger values of $x$.\n3. Since we want to include both tails, we need to multiply the area \n(probability) returned by pnorm by 2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwosidedpnorm = function(x,mu=0,sd=1) {\n    2*pnorm(abs(x),mu,sd,lower.tail=FALSE)\n}\n```\n:::\n\n\nAnd we can test this to see how likely it is to be 2 or 3 standard deviations\nfrom the mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwosidedpnorm(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04550026\n```\n\n\n:::\n\n```{.r .cell-code}\ntwosidedpnorm(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002699796\n```\n\n\n:::\n:::\n\n\n## The t-distribution\n\nWe spent time above working with z-scores and probability. An important\naspect of working with the normal distribution is that we MUST assume\nthat we know the standard deviation. Remember that the Z-score is defined\nas:\n\n$$Z = \\frac{x - \\mu}{\\sigma}$$\n\nThe formula for the *population*\nstandard deviation is:\n\n$$\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({xi - \\mu)^2}}$$ {#eq-pop-sd}\n\nIn general, the population standard deviation is taken as \"known\" as\nwe did above. \n\nIf we do not but only have a *sample*\nfrom the population, instead of using the Z-score, we use the t-score \ndefined as:\n\n$$t = \\frac{x - \\bar{x}}{s}$$ {#eq-tscore}\n\nThis looks quite similar to the formula for Z-score, but here we have to\n*estimate* the standard deviation, $s$ from the data. The formula for $s$\nis:\n\n$$s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}({x_{i} - \\bar{x})^2}}$$ {#eq-sample-sd}\n\nSince we are estimating the standard deviation from the data, this leads\nto extra variability that shows up as \"fatter tails\" for smaller sample\nsizes than for larger sample sizes. We can see this by comparing the\n_t-distribution_ for various numbers of degrees of freedom (sample sizes).\n\nWe can look at the effect of sample size on the distributions graphically\nby looking at the densities for 3, 5, 10, 20 degrees of freedom and\nthe normal distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nt_values = seq(-6,6,0.01)\ndf = data.frame(\n    value = t_values,\n    t_3   = dt(t_values,3),\n    t_6   = dt(t_values,6),\n    t_10  = dt(t_values,10),\n    t_20  = dt(t_values,20),\n    Normal= dnorm(t_values)\n) |>\n    tidyr::gather(\"Distribution\", \"density\", -value)\nggplot(df, aes(x=value, y=density, color=Distribution)) + \n    geom_line()\n```\n\n::: {.cell-output-display}\n![t-distributions for various degrees of freedom. Note that the tails are fatter for smaller degrees of freedom, which is a result of estimating the standard deviation from the data.](t-stats-and-tests_files/figure-pdf/fig-t-vs-z-1.pdf){#fig-t-vs-z fig-pos='H'}\n:::\n:::\n\n\nThe `dt` and `dnorm` functions give the density of the distributions\nfor each point. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf2 = df |> \n    group_by(Distribution) |>\n    arrange(value) |> \n    mutate(cdf=cumsum(density))\nggplot(df2, aes(x=value, y=cdf, color=Distribution)) + \n    geom_line()\n```\n\n::: {.cell-output-display}\n![](t-stats-and-tests_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### p-values based on Z vs t\n\nWhen we have a \"sample\" of data and want to compute the statistical \nsignificance of the difference of the mean from the population mean, \nwe calculate the standard deviation of the sample means (standard error).\n\n$$z = \\frac{x - \\mu}{\\sigma/\\sqrt{n}}$$\n\nLet's look at the relationship between the p-values of Z (from the normal distribution) vs t for a **sample** of data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5432)\nsamp = rnorm(5,mean = 0.5)\nz = sqrt(length(samp)) * mean(samp) #simplifying assumption (sigma=1, mu=0)\n```\n:::\n\n\nAnd the p-value if we assume we know the standard deviation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(z, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02428316\n```\n\n\n:::\n:::\n\n\nIn reality, we don't know the standard deviation, so we have to estimate it\nfrom the data. We can do this by calculating the sample standard deviation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nts = sqrt(length(samp)) * mean(samp) / sd(samp)\npnorm(ts, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0167297\n```\n\n\n:::\n\n```{.r .cell-code}\npt(ts,df = length(samp)-1, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0503001\n```\n\n\n:::\n:::\n\n\n### Experiment\n\nWhen sampling from a normal distribution, we often calculate p-values to test\nhypotheses or determine the statistical significance of our results. The\np-value represents the probability of obtaining a test statistic as extreme or\nmore extreme than the one observed, under the null hypothesis.\n\nIn a typical scenario, we assume that the population mean and standard\ndeviation are known. However, in many real-life situations, we don't know the\ntrue population standard deviation, and we have to estimate it using the sample\nstandard deviation (@eq-sample-sd). This estimation introduces some uncertainty into our\ncalculations, which affects the p-values. When we include an estimate of the standard deviation, we switch from using the\nstandard normal (z) distribution to the t-distribution for calculating\np-values. \n\nWhat would happen if we used the normal distribution to calculate p-values\nwhen we use the sample standard deviation? Let's find out!\n\n1. Simulate a bunch of samples of size `n` from the standard normal distribution\n2. Calculate the p-value distribution for those samples based on the \nnormal.\n3. Calculate the p-value distribution for those samples based on the \nnormal, but with the *estimated* standard deviation.\n4. Calculate the p-value distribution for those samples based on the \nt-distribution.\n\n\nCreate a function that draws a sample of size `n` from the standard\nnormal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzf = function(n) {\n    samp = rnorm(n)\n    z = sqrt(length(samp)) * mean(samp) / 1 #simplifying assumption (sigma=1, mu=0)\n    z\n}\n```\n:::\n\n\nAnd give it a try:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzf(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7406094\n```\n\n\n:::\n:::\n\n\nPerform 10000 replicates of our sampling and z-scoring. We are using the assumption\nthat we know the population standard deviation; in this case, we do know since we \nare sampling from the standard normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz10k = replicate(10000,zf(5))\nhist(pnorm(z10k))\n```\n\n::: {.cell-output-display}\n![](t-stats-and-tests_files/figure-pdf/unnamed-chunk-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAnd do the same, but now creating a t-score function. We are using the assumption\nthat we *don't* know the population standard deviation; in this case, we must\nestimate it from the data. Note the difference in the calculation of the t-score (`ts`)\nas compared to the z-score (`z`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf = function(n) {\n    samp = rnorm(n)\n    # now, using the sample standard deviation since we \n    # \"don't know\" the population standard deviation\n    ts = sqrt(length(samp)) * mean(samp) / sd(samp)\n    ts\n}\n```\n:::\n\n\nIf we use those t-scores and calculate the p-values based on the normal distribution, the \nhistogram of those p-values looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt10k = replicate(10000,tf(5))\nhist(pnorm(t10k))\n```\n\n::: {.cell-output-display}\n![](t-stats-and-tests_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nSince we are using the normal distribution to calculate the p-values, we are, in effect,\nassuming that we know the population standard deviation. This assumption is incorrect,\nand we can see that the p-values are not uniformly distributed between 0 and 1.\n\nIf we use those t-scores and calculate the p-values based on the t-distribution, the\nhistogram of those p-values looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(pt(t10k,5))\n```\n\n::: {.cell-output-display}\n![](t-stats-and-tests_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nNow, the p-values are uniformly distributed between 0 and 1, as expected.\n\nWhat is a qqplot and how do we use it? A qqplot is a plot of the quantiles of\ntwo distributions against each other. If the two distributions are identical,\nthe points will fall on a straight line. If the two distributions are different,\nthe points will deviate from the straight line. We can use a qqplot to compare\nthe t-distribution to the normal distribution. If the t-distribution is\nidentical to the normal distribution, the points will fall on a straight line.\nIf the t-distribution is different from the normal distribution, the points\nwill deviate from the straight line. In this case, we can see that the\nt-distribution is different from the normal distribution, as the points deviate\nfrom the straight line. What would happen if we increased the sample size? The\nt-distribution would approach the normal distribution, and the points would\nfall closer and closer to the straight line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqplot(z10k,t10k)\nabline(0,1)\n```\n\n::: {.cell-output-display}\n![](t-stats-and-tests_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Summary of t-distribution vs normal distribution\n\nThe t-distribution is a family of probability distributions that depends on a\nparameter called degrees of freedom, which is related to the sample size. The\nt-distribution approaches the standard normal distribution as the sample size\nincreases but has heavier tails for smaller sample sizes. This means that the\nt-distribution is more conservative in calculating p-values for small samples,\nmaking it harder to reject the null hypothesis. Including an estimate of the\nstandard deviation changes the way we calculate p-values by switching from the\nstandard normal distribution to the t-distribution, which accounts for the\nuncertainty introduced by estimating the population standard deviation from the\nsample. This adjustment is particularly important for small sample sizes, as it\nprovides a more accurate assessment of the statistical significance of our\nresults.\n\n\n## t.test\n\n### One-sample\n\nWe are going to use the `t.test` function to perform a one-sample t-test. The \n`t.test` function takes a vector of values as input that represents the sample \nvalues. In this case, we'll simulate our sample using the `rnorm` function and\npresume that our \"effect-size\" is 1. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm(20,1)\n# small sample\n# Just use the first 5 values of the sample\nt.test(x[1:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  x[1:5]\nt = 0.97599, df = 4, p-value = 0.3843\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.029600  2.145843\nsample estimates:\nmean of x \n0.5581214 \n```\n\n\n:::\n:::\n\n\nIn this case, we set up the experiment so that the null hypothesis is true (the \ntrue mean is not zero, but actually 1). However, we only have a small sample size\nthat leads to a modest p-value. \n\nIncreasing the sample size allows us to see the effect more clearly. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x[1:20])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  x[1:20]\nt = 3.8245, df = 19, p-value = 0.001144\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3541055 1.2101894\nsample estimates:\nmean of x \n0.7821474 \n```\n\n\n:::\n:::\n\n\n### two-sample\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm(10,0.5)\ny = rnorm(10,-0.5)\nt.test(x,y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  x and y\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\n mean of x  mean of y \n 0.7039205 -0.7968502 \n```\n\n\n:::\n:::\n\n\n### from a data.frame\n\nIn some situations, you may have data and groups as columns in a data.frame.\nSee the following data.frame, for example\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = data.frame(value=c(x,y),group=as.factor(rep(c('g1','g2'),each=10)))\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         value group\n1   1.12896674    g1\n2  -1.26838101    g1\n3   1.04577597    g1\n4   1.69075585    g1\n5   0.18672204    g1\n6   1.99715092    g1\n7   1.15424947    g1\n8   0.37671442    g1\n9  -0.09565723    g1\n10  0.82290783    g1\n11 -1.48530261    g2\n12 -1.29200440    g2\n13 -0.18778362    g2\n14  0.59205742    g2\n15 -2.10065248    g2\n16 -0.29961560    g2\n17 -0.38985115    g2\n18 -2.47126235    g2\n19 -0.63654380    g2\n20  0.30245611    g2\n```\n\n\n:::\n:::\n\n\nR allows us to perform a t-test using the `formula` notation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ group, data=df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n```\n\n\n:::\n:::\n\n\nYou read that as `value` **is a function of** `group`. In practice, this will\ndo a t-test between the `values` in `g1` vs `g2`.\n\n\n### Equivalence to linear model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ group, data=df, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 18, p-value = 0.002989\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5814078 2.4201337\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n```\n\n\n:::\n:::\n\n\nThis is *equivalent* to:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres = lm(value ~ group, data=df)\nsummary(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = value ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9723 -0.5600  0.2511  0.5252  1.3889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   0.7039     0.3094   2.275  0.03538 * \ngroupg2      -1.5008     0.4376  -3.430  0.00299 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9785 on 18 degrees of freedom\nMultiple R-squared:  0.3952,\tAdjusted R-squared:  0.3616 \nF-statistic: 11.76 on 1 and 18 DF,  p-value: 0.002989\n```\n\n\n:::\n:::\n\n\n## Power calculations\n\nThe power of a statistical test is the probability that the test will reject\nthe null hypothesis when the alternative hypothesis is true. In other words,\nthe power of a statistical test is the probability of not making a Type II\nerror. The power of a statistical test depends on the significance level\n(alpha), the sample size, and the effect size.\n\nThe `power.t.test` function can be used to calculate the power of a\none-sample t-test. \n\nLooking at `help(\"power.t.test\")`, we see that the function takes the following\narguments:\n\n* `n` - sample size\n* `delta` - effect size\n* `sd` - standard deviation of the sample\n* `sig.level` - significance level\n* `power` - power\n\nWe need to supply four of these arguments to calculate the fifth. For example,\nif we want to calculate the power of a one-sample t-test with a sample size of\n5, a standard deviation of 1, and an effect size of 1, we can use the\nfollowing command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 5\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.2859276\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\nThis gives a nice summary of the power calculation. We can also extract the\npower value from the result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample')$power\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4013203\n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\nWhen getting results from a function that don't look \"computable\" such as \nthose from `power.t.test`, you can use the `$` operator to extract the\nvalue you want. In this case, we want the `power` value from the result\nof `power.t.test`. \n\nHow would you know what to extract? You can use the `names` function\nor the `str` function to see the structure of the result. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"n\"           \"delta\"       \"sd\"          \"sig.level\"   \"power\"      \n[6] \"alternative\" \"note\"        \"method\"     \n```\n\n\n:::\n\n```{.r .cell-code}\n# or \nstr(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 8\n $ n          : num 5\n $ delta      : num 1\n $ sd         : num 1\n $ sig.level  : num 0.05\n $ power      : num 0.401\n $ alternative: chr \"two.sided\"\n $ note       : NULL\n $ method     : chr \"One-sample t test power calculation\"\n - attr(*, \"class\")= chr \"power.htest\"\n```\n\n\n:::\n:::\n\n:::\n\nAlternatively, we may know a lot about our experimental system and want to \ncalculate the sample size needed to achieve a certain power. For example, if\nwe want to achieve a power of 0.8 with a standard deviation of 1 and an effect\nsize of 1, we can use the following command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(delta = 1, sd = 1, sig.level = 0.05, power = 0.8, type = \"one.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 9.937864\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\nThe power.t.test function is convenient and quite fast. As we've seen before, though,\nsometimes the distribution of the test statistics is now easily calculated. In those\ncases, we can use simulation to calculate the power of a statistical test. For example,\nif we want to calculate the power of a one-sample t-test with a sample size of 5, a\nstandard deviation of 1, and an effect size of 1, we can use the following command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_t_test_pval <- function(n = 5, delta = 1, sd = 1, sig.level = 0.05) {\n    x = rnorm(n, delta, sd)\n    t.test(x)$p.value <= sig.level\n}\npow = mean(replicate(1000, sim_t_test_pval()))\npow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.405\n```\n\n\n:::\n:::\n\n\nLet's break this down. First, we define a function called `sim_t_test_pval` that\ntakes the same arguments as the `power.t.test` function. Inside the function, we\nsimulate a sample of size `n` from a normal distribution with mean `delta` and\nstandard deviation `sd`. Then, we perform a one-sample t-test on the sample and\nreturn a logical value indicating whether the p-value is less than the\nsignificance level. Next, we use the `replicate` function to repeat the\nsimulation 1000 times. Finally, we calculate the proportion of simulations in\nwhich the p-value was less than the significance level. This proportion is an\nestimate of the power of the one-sample t-test.\n\nLet's compare the results of the `power.t.test` function and our simulation-based\napproach:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05, type='one.sample')$power\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4013203\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(replicate(1000, sim_t_test_pval(n = 5, delta = 1, sd = 1, sig.level = 0.05)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.414\n```\n\n\n:::\n:::\n\n\n## Resources\n\nSee the [pwr package](https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html) for more information on power calculations.\n\n",
    "supporting": [
      "t-stats-and-tests_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}