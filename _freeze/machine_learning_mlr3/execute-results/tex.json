{
  "hash": "d9e648a0ab56b26a8b0f16accbe4dffa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Machine Learning 2\"\n---\n\n## Overview\n\nIn this chapter, we focus on practical aspects of machine learning. \nThe goal is to provide a hands-on introduction to the application of machine learning techniques to real-world data. \nWhile the theoretical foundations of machine learning are important, the ability to apply these techniques to solve practical problems is equally crucial.\nIn this chapter, we will use the `mlr3` package in R to build and evaluate machine learning models for classification and regression tasks. \n\nWe will use three examples to illustrate the machine learning workflow:\n\n1. **Cancer types classification**: We will classify different types of cancer based on gene expression data.\n2. **Age prediction from DNA methylation**: We will predict the chronological age of individuals based on DNA methylation patterns.\n3. **Gene expression prediction**: We will predict gene expression levels based on histone modification data.\n\nWe'll be applying knn, decision trees, and random forests, linear regression, and penalized regression models to these datasets.\n\nThe mlr3 R package is a modern, object-oriented machine learning framework in R\nthat builds on the success of its predecessor, the mlr package. It provides a\nflexible and extensible platform for handling common machine learning tasks\nsuch as data preprocessing, model training, hyperparameter tuning, and model\nevaluation @fig-mlr3-ecosystem. The package is designed to guide and standardize \nthe process of using complex machine learning pipelines.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The mlr3 ecosystem.](images/mlr3_ecosystem.png){#fig-mlr3-ecosystem width=11.17in}\n:::\n:::\n\n\n### Key features of mlr3\n\n* Task abstraction \n  : mlr3 encapsulates different types of learning problems\n  like classification, regression, and survival analysis into \"Task\" objects,\n  making it easier to handle various learning scenarios. Examples of tasks \n  include classification tasks, regression tasks, and survival tasks.\n* Modular design\n  : The package follows a modular design, allowing users to quickly swap out\n  different components such as learners (algorithms), measures (performance\n  metrics), and resampling strategies. Examples of learners include linear\n  regression, logistic regression, and random forests. Examples of measures\n  include accuracy, precision, recall, and F1 score. Examples of resampling\n  strategies include cross-validation, bootstrapping, and holdout validation.\n* Extensibility\n  : Users can extend the functionality of mlr3 by adding custom components like\n  learners, measures, and preprocessing steps via the R6 object-oriented\n  system.\n* Preprocessing\n  : mlr3 provides a flexible way to preprocess data using \"PipeOps\" (pipeline\n  operations), allowing users to create reusable preprocessing pipelines.\n* Tuning and model selection\n  : mlr3 supports hyperparameter tuning and model selection using various\n  search strategies like grid search, random search, and Bayesian optimization.\n* Parallelization\n  : The package allows for parallelization of model training and evaluation,\n  making it suitable for large-scale machine learning tasks.\n* Benchmarking\n  : mlr3 facilitates benchmarking of multiple algorithms on multiple tasks,\n  simplifying the process of comparing and selecting the best models.\n\nYou can find more information, including tutorials and examples, on the\nofficial mlr3 GitHub repository^[<https://github.com/mlr-org/mlr3>] and the mlr3\nbook^[<https://mlr3book.mlr-org.com/>].\n\n\n## The mlr3 workflow\n\nThe mlr3 package is designed to simplify the process of creating and deploying\ncomplex machine learning pipelines. The package follows a modular design, which\nmeans that users can quickly swap out different components such as learners\n(algorithms), measures (performance metrics), and resampling strategies. The\npackage also supports parallelization of model training and evaluation, making\nit suitable for large-scale machine learning tasks.\n\n```{mermaid}\n%%| label: fig-mlr3-workflow\n%%| fig-cap: \"The simplified workflow of a machine learning pipeline using mlr3.\"\ngraph TD\n  A[Load data]\n  B[Create a task\\nregression, classification, etc.]\n  B --> learner[Choose learner]\n  A --> C[Split data]\n  C --> trainset(Training set)\n  C --> testset[Test set]\n  trainset --> train[Train model]\n  learner --> train\n  testset --> testing[Test model]\n  train --> testing\n  testing --> eval[Evaluate &\\nInterpret]\n```\n\nThe following sections describe each of these steps in detail.\n\n### The machine learning Task\n\nImagine you want to teach a computer how to make predictions or decisions, similar to how you might teach a student. To do this, you need to clearly define what you want the computer to learn and work on. This is called defining a \"task.\" Let's break down what this involves and why it's important.\n\n#### Step 1: Understand the Problem\n\nFirst, think about what problem you want to solve or what question you want the computer to answer. For example:\n- Do you want to predict the weather for tomorrow?\n- Are you trying to figure out if an email is spam or not?\n- Do you want to know how much a house might sell for?\n\nThese questions define your **task type**. In machine learning, there are several common task types:\n\n- **Classification:** Deciding which category something belongs to (e.g., spam or not spam).\n- **Regression:** Predicting a number (e.g., the price of a house).\n- **Clustering:** Grouping similar items together (e.g., customer segmentation).\n\n#### Step 2: Choose Your Data\n\nNext, you need data that is related to your problem. Think of data as the information or examples you'll use to teach the computer. For instance, if your task is to predict house prices, your data might include:\n\n- The size of the house\n- The number of bedrooms\n- The location of the house\n- The age of the house\n\nThese pieces of information are called **features**. Features are the input that the computer uses to make predictions.\n\n#### Step 3: Define the Target\n\nAlong with features, you need to define the **target**. The target is what you want to predict or decide. In the house price example, the target would be the actual price of the house.\n\n#### Step 4: Create the Task\n\nNow that you have your problem, data, and target, you can create the task. In mlr3, a task brings together the type of problem (task type), the features (input data), and the target (what you want to predict).\n\nHere's a simple summary:\n\n1. **Task Type:** What kind of problem are you solving? (e.g., classification, regression)\n2. **Features:** What information do you have to make the prediction? (e.g., size, location)\n3. **Target:** What are you trying to predict? (e.g., house price)\n\nBy clearly defining these elements, you set a solid foundation for the machine learning process. This helps ensure that the computer can learn effectively and make accurate predictions.\n\n#### mlr3 and Tasks\n\nThe mlr3 package uses the concept of \"Tasks\" to encapsulate different types of\nlearning problems like classification, regression, and survival analysis. A Task\ncontains the data (features and target variable) and additional metadata to\ndefine the machine learning problem. For example, in a classification task, the\ntarget variable is a label (stored as a character or factor), while in a\nregression task, the target variable is a numeric quantity (stored as an integer\nor numeric).\n\nThere are a number of [Task Types](https://mlr3book.mlr-org.com/02-basics-tasks.html#tasks-types) that \nare supported by mlr3. To create a task from a [`data.frame()`](https://www.rdocumentation.org/packages/base/topics/data.frame), [`data.table()`](https://www.rdocumentation.org/packages/data.table/topics/data.table-package) or [`Matrix()`](https://www.rdocumentation.org/packages/Matrix/topics/Matrix), you first need to select the right task type:\n\n-   **Classification Task**: The target is a label (stored as `character` or `factor`) with only relatively few distinct values → [`TaskClassif`](https://mlr3.mlr-org.com/reference/TaskClassif.html).\n    \n-   **Regression Task**: The target is a numeric quantity (stored as `integer` or `numeric`) → [`TaskRegr`](https://mlr3.mlr-org.com/reference/TaskRegr.html).\n    \n-   **Survival Task**: The target is the (right-censored) time to an event. More censoring types are currently in development → [`mlr3proba::TaskSurv`](https://mlr3proba.mlr-org.com/reference/TaskSurv.html) in add-on package [mlr3proba](https://mlr3proba.mlr-org.com/).\n    \n-   **Density Task**: An unsupervised task to estimate the density → [`mlr3proba::TaskDens`](https://mlr3proba.mlr-org.com/reference/TaskDens.html) in add-on package [mlr3proba](https://mlr3proba.mlr-org.com/).\n    \n-   **Cluster Task**: An unsupervised task type; there is no target and the aim is to identify similar groups within the feature space → [`mlr3cluster::TaskClust`](https://mlr3cluster.mlr-org.com/reference/TaskClust.html) in add-on package [mlr3cluster](https://mlr3cluster.mlr-org.com/).\n    \n-   **Spatial Task**: Observations in the task have spatio-temporal information (e.g. coordinates) → [`mlr3spatiotempcv::TaskRegrST`](https://mlr3spatiotempcv.mlr-org.com/reference/TaskRegrST.html) or [`mlr3spatiotempcv::TaskClassifST`](https://mlr3spatiotempcv.mlr-org.com/reference/TaskClassifST.html) in add-on package [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com/).\n    \n-   **Ordinal Regression Task**: The target is ordinal → `TaskOrdinal` in add-on package [mlr3ordinal](https://github.com/mlr-org/mlr3ordinal) (still in development).\n\n### The \"Learner\" in Machine Learning\n\nAfter you've defined your task, the next step in teaching a computer to make predictions or decisions is to choose a \"learner.\" Let's explore what a learner is and how it fits into the mlr3 package.\n\n#### What is a \"Learner\"?\n\nThink of a learner as the method or tool that the computer uses to learn from the data. Another common name for a \"learner\" is a \"model.\" It's similar to choosing a tutor or a teacher for a student. Different learners have different ways of understanding and processing information. For example:\n\n- Some learners might be great at recognizing patterns in data, like a tutor who is excellent at spotting trends.\n- Others might be good at making decisions based on rules, like a tutor who uses step-by-step logic.\n\nIn machine learning, there are many types of learners, each with its own strengths and weaknesses. Here are a few examples:\n\n- **Decision Trees:** These learners make decisions by asking a series of questions, like \"Is the house larger than 1000 square feet?\" and \"Does it have more than 3 bedrooms?\"\n- **k-Nearest Neighbors:** These learners make predictions based on the similarity of new data points to existing data points.\n- **Linear Regression:** This learner tries to fit a straight line through the data points to make predictions about numbers.\n- **Random Forests:** These are like a group of decision trees working together to make more accurate predictions.\n- **Support Vector Machines:** These learners find the best boundary that separates different categories in the data.\n\n#### Choosing the Right Learner\n\nSelecting the right learner is crucial because different learners work better for different types of tasks and data. For example:\n\n- If your task is to classify emails as spam or not spam, a decision tree or a support vector machine might work well.\n- If you're predicting house prices, linear regression or random forests could be good choices.\n\nThe goal is to find a learner that can understand the patterns in your data and make accurate predictions. This is where the mlr3 package comes in handy. It provides a wide range of learners that you can choose from, making it easier to experiment and find the best learner for your task.\n\n#### Learners in mlr3\n\nIn the mlr3 package, learners are pre-built tools that you can easily use for your tasks. Here's how it works:\n\n1. **Select a Learner:** mlr3 provides a variety of learners to choose from, like decision trees, linear regression, and more.\n2. **Train the Learner:** Once you've selected a learner, you provide it with your task (the problem, data, and target). The learner uses this information to learn and make predictions.\n3. **Evaluate and Improve:** After training, you can test how well the learner performs and make adjustments if needed, such as trying a different learner or fine-tuning the current one.\n\n#### mlr3 and Learners\n\n\nObjects of class [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) provide a unified interface to many popular machine learning algorithms in R. They consist of methods to train and predict a model for a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and provide meta-information about the learners, such as the hyperparameters (which control the behavior of the learner) you can set.\n\nThe base class of each learner is [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html), specialized for regression as [`LearnerRegr`](https://mlr3.mlr-org.com/reference/LearnerRegr.html) and for classification as [`LearnerClassif`](https://mlr3.mlr-org.com/reference/LearnerClassif.html). Other types of learners, provided by extension packages, also inherit from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) base class, e.g. [`mlr3proba::LearnerSurv`](https://mlr3proba.mlr-org.com/reference/LearnerSurv.html) or [`mlr3cluster::LearnerClust`](https://mlr3cluster.mlr-org.com/reference/LearnerClust.html).\n\nAll Learners work in a two-stage procedure:\n\n![Two stages of a learner. Top: data (features and a target) are passed to an (untrained) learner. Bottom: new data are passed to the trained model which makes predictions for the ‘missing’ target column.](images/mlr3_learner_stages.png){#fig-mlr3-learner-stages}\n\n-   **Training stage**: The training data (features and target) is passed to the Learner’s `$train()` function which trains and stores a model, i.e. the relationship of the target and features.\n-   **Predict stage**: The new data, usually a different slice of the original data than used for training, is passed to the `$predict()` method of the Learner. The model trained in the first step is used to predict the missing target, e.g. labels for classification problems or the numerical value for regression problems.\n\nThere are a number of [predefined learners](https://mlr3book.mlr-org.com/02-basics-learners.html#predefined-learners). The [mlr3](https://mlr3.mlr-org.com/) package ships with the following set of classification and regression learners. We deliberately keep this small to avoid unnecessary dependencies:\n\n-   [`classif.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.featureless.html): Simple baseline classification learner. The default is to always predict the label that is most frequent in the training set. While this is not very useful by itself, it can be used as a “[fallback learner](https://mlr3book.mlr-org.com/fallback-learners)” to make predictions in case another, more sophisticated, learner failed for some reason.\n-   [`regr.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_regr.featureless.html): Simple baseline regression learner. The default is to always predict the mean of the target in training set. Similar to [`mlr_learners_classif.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.featureless.html), it makes for a good “[fallback learner](https://mlr3book.mlr-org.com/fallback-learners)”\n-   [`classif.rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html): Single classification tree from package [rpart](https://cran.r-project.org/package=rpart).\n-   [`regr.rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_regr.rpart.html): Single regression tree from package [rpart](https://cran.r-project.org/package=rpart).\n\nThis set of baseline learners is usually insufficient for a real data analysis. Thus, we have cherry-picked implementations of the most popular machine learning method and collected them in the [mlr3learners](https://mlr3learners.mlr-org.com/) package:\n\n-   Linear ([`regr.lm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.lm.html)) and logistic ([`classif.log_reg`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.log_reg.html)) regression\n-   Penalized Generalized Linear Models ([`regr.glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.glmnet.html), [`classif.glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.glmnet.html)), possibly with built-in optimization of the penalization parameter ([`regr.cv_glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.cv_glmnet.html), [`classif.cv_glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.cv_glmnet.html))\n-   (Kernelized) k\\-Nearest Neighbors regression ([`regr.kknn`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.kknn.html)) and classification ([`classif.kknn`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.kknn.html)).\n-   Kriging / Gaussian Process Regression ([`regr.km`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.km.html))\n-   Linear ([`classif.lda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.lda.html)) and Quadratic ([`classif.qda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.qda.html)) Discriminant Analysis\n-   Naive Bayes Classification ([`classif.naive_bayes`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.naive_bayes.html))\n-   Support-Vector machines ([`regr.svm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.svm.html), [`classif.svm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.svm.html))\n-   Gradient Boosting ([`regr.xgboost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.xgboost.html), [`classif.xgboost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html))\n-   Random Forests for regression and classification ([`regr.ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html), [`classif.ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html))\n\nMore machine learning methods and alternative implementations are collected in the [mlr3extralearners repository](https://github.com/mlr-org/mlr3extralearners/).\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(GEOquery)\nlibrary(mlr3learners) # for knn\nlibrary(ranger) # for randomforest\nset.seed(789)\n```\n:::\n\n\n## Example: Cancer types\n\nIn this exercise, we will be classifying cancer types based on gene \nexpression data. The data we are going to access are from @brouwer-visser_regulatory_2018.\n\nThe data are from the Gene Expression Omnibus (GEO) database, a public repository of functional genomics data. The data are from a study that aimed to identify gene expression signatures that can distinguish between different types of cancer. The data include gene expression profiles from patients with different types of cancer. The goal is to build a machine learning model that can predict the cancer type based on the gene expression data.\n\n### Understanding the Problem\n\nBefore we start building a machine learning model, it's important to understand the problem we are trying to solve. Here are some key questions to consider:\n\n- What are the features?\n- What is the target variable?\n- What type of machine learning task is this (classification, regression, clustering)?\n- What is the goal of the analysis?\n\n### Data Preparation\n\nUse the [GEOquery] package to fetch data about [GSE103512].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GEOquery)\ngse = getGEO(\"GSE103512\")[[1]]\n```\n:::\n\n\nThe first step, a detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the `ExpressionSet`, to the newer `SummarizedExperiment`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SummarizedExperiment)\nse = as(gse, \"SummarizedExperiment\")\n```\n:::\n\n\nExamine two variables of interest, cancer type and tumor/normal status.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(colData(se),table(`cancer.type.ch1`,`normal.ch1`))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               normal.ch1\ncancer.type.ch1 no yes\n          BC    65  10\n          CRC   57  12\n          NSCLC 60   9\n          PCA   60   7\n```\n\n\n:::\n:::\n\n\nBefore embarking on a machine learning analysis, we need to make sure that\nwe understand the data. Things like missing values, outliers, and other\nproblems can cause problems for machine learning algorithms. \n\nIn R, plotting, summaries, and other exploratory data analysis tools are\navailable. PCA analysis, clustering, and other methods can also be used\nto understand the data. It is worth spending time on this step, as it\ncan save time later.\n\n### Feature selection and data cleaning\n\nWhile we could use all genes in the analysis, we will select the most informative genes\nusing the variance of gene expression across samples. Other methods for feature selection\nare available, including those based on correlation with the outcome variable. \n\n\n::: {.callout-important}\n## Feature selection\n\nFeature selection should be done on the training data only, not the test data to avoid\noverfitting. The test data should be used only for evaluation. In other words, the test\ndata should be \"unseen\" by the model until the final evaluation.\n:::\n\nRemember that the `apply` function applies a function to each row or column of\na matrix. Here, we apply the `sd` function to each row of the expression matrix\nto get a vector of stan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsds = apply(assay(se, 'exprs'),1,sd)\n## filter out normal tissues\nse_small = se[order(sds,decreasing = TRUE)[1:200],\n              colData(se)$characteristics_ch1.1=='normal: no']\n# remove genes with no gene symbol\nse_small = se_small[rowData(se_small)$Gene.Symbol!='',]\n```\n:::\n\n\nTo make the data easier to work with, we will use the opportunity to \nuse one of the rowData columns as the rownames of the data frame.\nThe `make.names` function is used to make sure that the rownames are\nvalid R variable names and unique.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## convert to matrix for later use\ndat = assay(se_small, 'exprs')\nrownames(dat) = make.names(rowData(se_small)$Gene.Symbol)\n```\n:::\n\n\nWe also need to transpose the data so that the rows are the samples and the\ncolumns are the features in order to use the data with mlr3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeat_dat = t(dat)\ntumor = data.frame(tumor_type = colData(se_small)$cancer.type.ch1, feat_dat)\n```\n:::\n\n\nThis is another good time to check the data. Make sure that the data is in the\nformat that you expect. Check the dimensions, the column names, and the data\ntypes. \n\n\n### Creating the \"task\"\n\nThe first step in using mlr3 is to create a [task](https://mlr3book.mlr-org.com/02-basics-tasks.html).\nA task is a data set with a target variable. In this case, the target variable\nis the cancer type. The mlr3 package provides a function to convert a data frame\ninto a task. These tasks can be used with any machine learning algorithm in mlr3.\n\nThis is a classification task, so we will use the `as_task_classif` function to\ncreate the task. The classification task requires a target variable that is categorical.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3)\ntumor$tumor_type = as.factor(tumor$tumor_type)\ntask = as_task_classif(tumor,target='tumor_type')\n```\n:::\n\n\n### Splitting the data\n\nHere, we randomly divide the data into 2/3 training data and 1/3 test data.\nThis is a common split, but other splits can be used. The training data is used\nto train the model, and the test data is used to evaluate the trained model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n```\n:::\n\n\n::: {.callout-important}\nTraining and testing on the same data is a common mistake. We want to test\nthe model on data that it has not seen before. This is the only way to know\nif the model is overfitting and to get an accurate estimate of the model's\nperformance.\n:::\n\nIn the next sections, we will train and evaluate three different models on the\ndata: k-nearest-neighbor, classification tree, and random forest. Remember that\nthe goal is to predict the cancer type based on the gene expression data.\nThe mlr3 package uses the concept of \"learners\" to encapsulate different machine\nlearning algorithms. \n\n### Example learners\n\n#### K-nearest-neighbor\n\nThe first model we will use is the k-nearest-neighbor model. This model\nis based on the idea that similar samples have similar outcomes. The\nnumber of neighbors to use is a parameter that can be tuned. We'll use\nthe default value of 7, but you can try other values to see how they\naffect the results. In fact, mlr3 provides the ability to tune parameters\nautomatically, but we won't cover that here.\n\n##### Create the learner\n\nIn mlr3, the machine learning algorithms are called learners. To create\na learner, we use the `lrn` function. The `lrn` function takes the name\nof the learner as an argument. The `lrn` function also takes other arguments\nthat are specific to the learner. In this case, we will use the default\nvalues for the arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3learners)\nlearner = lrn(\"classif.kknn\")\n```\n:::\n\n\nYou can get a list of all the learners available in mlr3 by using the `lrn()`\nfunction without any arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<DictionaryLearner> with 51 stored values\nKeys: classif.cv_glmnet, classif.debug, classif.featureless,\n  classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n  classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n  clust.agnes, clust.ap, clust.bico, clust.birch, clust.cmeans,\n  clust.cobweb, clust.dbscan, clust.dbscan_fpc, clust.diana, clust.em,\n  clust.fanny, clust.featureless, clust.ff, clust.hclust,\n  clust.hdbscan, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n  clust.mclust, clust.meanshift, clust.optics, clust.pam,\n  clust.SimpleKMeans, clust.xmeans, regr.cv_glmnet, regr.debug,\n  regr.featureless, regr.glmnet, regr.kknn, regr.km, regr.lm,\n  regr.nnet, regr.ranger, regr.rpart, regr.svm, regr.xgboost\n```\n\n\n:::\n:::\n\n\n\n##### Train\n\nTo train the model, we use the `train` function. The `train` function takes\nthe task and the row ids of the training data as arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$train(task, row_ids = train_set)\n```\n:::\n\n\nHere, we can look at the trained model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# output is large, so do this on your own\nlearner$model\n```\n:::\n\n\n##### Predict\n\nLets use our trained model works to predict the classes of the \n**training** data. Of course, we already know the classes of the\ntraining data, but this is a good way to check that the model\nis working as expected. It also gives us a measure of performance\non the training data that we can compare to the test data to \nlook for overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train = learner$predict(task, row_ids=train_set)\n```\n:::\n\n\nAnd check on the test data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test = learner$predict(task, row_ids=test_set)\n```\n:::\n\n\n##### Assess\n\nIn this section, we can look at the accuracy and performance of our model \non the training data and the test data. We can also look at the confusion\nmatrix to see which classes are being confused with each other. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse BC CRC NSCLC PCA\n   BC    42   0     0   0\n   CRC    0  40     0   0\n   NSCLC  1   0    44   0\n   PCA    0   0     0  35\n```\n\n\n:::\n:::\n\n\nThis is a multi-class confusion matrix. The rows are the true classes and\nthe columns are the predicted classes. The diagonal shows the number of\nsamples that were correctly classified. The off-diagonal elements show\nthe number of samples that were misclassified.\n\nWe can also look at the accuracy of the model on the training data and the\ntest data. The accuracy is the number of correctly classified samples divided\nby the total number of samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n  0.9938272 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse BC CRC NSCLC PCA\n   BC    22   0     0   0\n   CRC    0  17     1   0\n   NSCLC  0   0    15   0\n   PCA    0   0     0  25\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n     0.9875 \n```\n\n\n:::\n:::\n\n\nCompare the accuracy on the training data to the accuracy on the test data. \nDo you see any evidence of overfitting?\n\n#### Classification tree\n\nWe are going to use a classification tree to classify the data. A classification\ntree is a series of yes/no questions that are used to classify the data. The\nquestions are based on the features in the data. The classification tree is\nbuilt by finding the feature that best separates the data into the different\nclasses. Then, the data is split based on the value of that feature. The\nprocess is repeated until the data is completely separated into the different\nclasses.\n\n##### Train\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# in this case, we want to keep the model\n# so we can look at it later\nlearner = lrn(\"classif.rpart\", keep_model = TRUE)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$train(task, row_ids = train_set)\n```\n:::\n\n\nWe can take a look at the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 162 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 162 118 NSCLC (0.26543210 0.24691358 0.27160494 0.21604938)  \n   2) CDHR5>=5.101625 40   0 CRC (0.00000000 1.00000000 0.00000000 0.00000000) *\n   3) CDHR5< 5.101625 122  78 NSCLC (0.35245902 0.00000000 0.36065574 0.28688525)  \n     6) ACPP< 6.088431 87  43 NSCLC (0.49425287 0.00000000 0.50574713 0.00000000)  \n      12) GATA3>=4.697803 41   1 BC (0.97560976 0.00000000 0.02439024 0.00000000) *\n      13) GATA3< 4.697803 46   3 NSCLC (0.06521739 0.00000000 0.93478261 0.00000000) *\n     7) ACPP>=6.088431 35   0 PCA (0.00000000 0.00000000 0.00000000 1.00000000) *\n```\n\n\n:::\n:::\n\n\nDecision trees are easy to visualize if they are small. Here, we can see that the tree\nis very simple, with only two splits. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\nlibrary(ggparty)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: ggplot2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: partykit\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: grid\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: libcoin\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mvtnorm\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'partykit'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:SummarizedExperiment':\n\n    width\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:GenomicRanges':\n\n    width\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:IRanges':\n\n    width\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:S4Vectors':\n\n    width\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:BiocGenerics':\n\n    width\n```\n\n\n:::\n\n```{.r .cell-code}\nautoplot(learner, type='ggparty')\n```\n\n::: {.cell-output-display}\n![](machine_learning_mlr3_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n##### Predict\n\nNow that we have trained the model on the _training_ data, we can use it to predict the classes of the training data and the test data. The `$predict` method takes a `task` and\nproduces a prediction based on the _trained_ model, in this case, called `learner`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train = learner$predict(task, row_ids=train_set)\n```\n:::\n\n\nRemember that we split the data into training and test sets. We can use the trained model to predict the classes of the test data. Since the _test_ data was not used to train the model, it is\nnot \"cheating\" like what we just did where we did the prediction on the _training_ data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test = learner$predict(task, row_ids=test_set)\n```\n:::\n\n\n##### Assess\n\nFor classification tasks, we often look at a confusion matrix of the _truth_ vs the _predicted_\nclasses for the samples. \n\n::: {.callout-important}\nAssessing the performance of a model should **always** be **reported** from assessment \non an independent test set. \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse BC CRC NSCLC PCA\n   BC    40   0     1   0\n   CRC    0  40     0   0\n   NSCLC  3   0    43   0\n   PCA    0   0     0  35\n```\n\n\n:::\n:::\n\n\n* What does this confusion matrix tell you?\n\nWe can also ask for several \"measures\" of the performance of the model. Here, we ask for the accuracy of the model. To get a complete list of measures, use `msr()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n  0.9753086 \n```\n\n\n:::\n:::\n\n* How does the accuracy compare to the confusion matrix?\n* How does this accuracy compare to the accuracy of the k-nearest-neighbor model?\n* How about the decision tree model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse BC CRC NSCLC PCA\n   BC    20   0     1   0\n   CRC    0  17     3   0\n   NSCLC  2   0    12   0\n   PCA    0   0     0  25\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n      0.925 \n```\n\n\n:::\n:::\n\n\n* What does the confusion matrix in the _test_ set tell you?\n* How do the assessments of the _test_ and _training_ sets differ? \n\n::: {.callout-tip}\n## Overfitting\n\nWhen the assessment of the test set is worse than the evaluation\nof the training set, the model may be _overfit_. How to address\noverfitting varies by model type, but it is a sign that you should\npay attention to model selection and parameters.\n:::\n\n\n#### RandomForest\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.ranger\", importance = \"impurity\")\n```\n:::\n\n\n##### Train\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$train(task, row_ids = train_set)\n```\n:::\n\nAgain, you can look at the model that was trained.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", importance = \"impurity\",      num.threads = 1L) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      162 \nNumber of independent variables:  192 \nMtry:                             13 \nTarget node size:                 1 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error:             0.62 % \n```\n\n\n:::\n:::\n\n\nFor more details, the mlr3 random forest approach is based\nont he ranger package. You can look at the ranger documentation.\n\n* What is the OOB error in the output? \n\nRandom forests are a collection of decision trees. Since predictors enter the trees\nin a random order, the trees are different from each other. The random forest \nprocedure gives us a measure of the \"importance\" of each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(learner$importance(), 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   CDHR5  TRPS1.1    FABP1   EPS8L3    KRT20    EFHD1   LGALS4    TRPS1 \n4.791870 3.918063 3.692649 3.651422 3.340382 3.314491 2.952969 2.926175 \n   SFTPB  SFTPB.1    GATA3  GATA3.1  TMPRSS2    MUC12    POF1B \n2.805811 2.681004 2.344603 2.271845 2.248734 2.207347 1.806906 \n```\n\n\n:::\n:::\n\nMore \"important\" variables are those that are more often used in the trees. \nAre the most important variables the same as the ones that were important\nin the decision tree?\n\nIf you are interested, look up a few of the important variables in the \nmodel to see if they make biological sense. \n\n##### Predict\n\nAgain, we can use the trained model to predict the classes of the training data and the test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train = learner$predict(task, row_ids=train_set)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test = learner$predict(task, row_ids=test_set)\n```\n:::\n\n\n##### Assess\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse BC CRC NSCLC PCA\n   BC    43   0     0   0\n   CRC    0  40     0   0\n   NSCLC  0   0    44   0\n   PCA    0   0     0  35\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n          1 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        truth\nresponse BC CRC NSCLC PCA\n   BC    22   0     0   0\n   CRC    0  17     0   0\n   NSCLC  0   0    16   0\n   PCA    0   0     0  25\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n          1 \n```\n\n\n:::\n:::\n\n\n## Example Predicting age from DNA methylation\n\nWe will be building a regression model for chronological age prediction, based on DNA methylation. This is based on the work of [Jana Naue et al. 2017](https://www.sciencedirect.com/science/article/pii/S1872497317301643?via%3Dihub), in which biomarkers are examined to predict the chronological age of humans by analyzing the DNA methylation patterns. Different machine learning algorithms are used in this study to make an age prediction.\n\nIt has been recognized that within each individual, the level of [DNA methylation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3174260/) changes with age. This knowledge is used to select useful biomarkers from DNA methylation datasets. The [CpG sites](https://en.wikipedia.org/wiki/CpG_site) with the highest correlation to age are selected as the biomarkers (and therefore features for building a regression model). In this tutorial, specific biomarkers are analyzed by machine learning algorithms to create an age prediction model.\n\nThe data are taken from [this tutorial](https://training.galaxyproject.org/training-material/topics/statistics/tutorials/regression_machinelearning/tutorial.html). \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nmeth_age = rbind(\n    fread('https://zenodo.org/record/2545213/files/test_rows_labels.csv'),\n    fread('https://zenodo.org/record/2545213/files/train_rows.csv')\n)\n```\n:::\n\n\nLet's take a quick look at the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(meth_age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   RPA2_3 ZYG11A_4  F5_2 HOXC4_1 NKIRAS2_2 MEIS1_1 SAMD10_2 GRM2_9 TRIM59_5\n    <num>    <num> <num>   <num>     <num>   <num>    <num>  <num>    <num>\n1:  65.96    18.08 41.57   55.46     30.69   63.42    40.86  68.88    44.32\n2:  66.83    20.27 40.55   49.67     29.53   30.47    37.73  53.30    50.09\n3:  50.30    11.74 40.17   33.85     23.39   58.83    38.84  35.08    35.90\n4:  65.54    15.56 33.56   36.79     20.23   56.39    41.75  50.37    41.46\n5:  59.01    14.38 41.95   30.30     24.99   54.40    37.38  30.35    31.28\n6:  81.30    14.68 35.91   50.20     26.57   32.37    32.30  55.19    42.21\n   LDB2_3 ELOVL2_6 DDO_1 KLF14_2   Age\n    <num>    <num> <num>   <num> <int>\n1:  56.17    62.29 40.99    2.30    40\n2:  58.40    61.10 49.73    1.07    44\n3:  58.81    50.38 63.03    0.95    28\n4:  58.05    50.58 62.13    1.99    37\n5:  65.80    48.74 41.88    0.90    24\n6:  70.15    61.36 33.62    1.87    43\n```\n\n\n:::\n:::\n\n\nAs before, we create the `task` object, but this time we use `as_task_regr()` to create a regression task.\n\n* Why is this a regression task?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask = as_task_regr(meth_age,target = 'Age')\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n```\n:::\n\n\n### Example learners\n\n#### Linear regression\n\nWe will start with a simple linear regression model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner = lrn(\"regr.lm\")\n```\n:::\n\n##### Train\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$train(task, row_ids = train_set)\n```\n:::\n\n\nWhen you train a linear regression model, we can evaluate some of the \ndiagnostic plots to see if the model is appropriate (@fig-regression-diagnostic).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(learner$model)\n```\n\n::: {.cell-output-display}\n![Regression diagnostic plots. The top left plot shows the residuals vs. fitted values. The top right plot shows the normal Q-Q plot. The bottom left plot shows the scale-location plot. The bottom right plot shows the residuals vs. leverage. ](machine_learning_mlr3_files/figure-pdf/fig-regression-diagnostic-1.pdf){#fig-regression-diagnostic fig-pos='H'}\n:::\n:::\n\n\n##### Predict\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train = learner$predict(task, row_ids=train_set)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test = learner$predict(task, row_ids=test_set)\n```\n:::\n\n\n##### Assess\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionRegr> for 209 observations:\n row_ids truth response\n     298    29 31.40565\n     103    58 56.26019\n     194    53 48.96480\n     ---   ---      ---\n     312    48 52.61195\n     246    66 67.66312\n     238    38 39.38414\n```\n\n\n:::\n:::\n\n\nWe can plot the relationship between the `truth` and `response`, or predicted\nvalue to see visually how our model performs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](machine_learning_mlr3_files/figure-pdf/unnamed-chunk-43-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe can use the r-squared of the fit to roughly compare two models. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq \n0.9376672 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionRegr> for 103 observations:\n row_ids truth response\n       4    37 37.64301\n       5    24 28.34777\n       7    34 33.22419\n     ---   ---      ---\n     306    42 41.65864\n     307    63 58.68486\n     309    68 70.41987\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq \n0.9363526 \n```\n\n\n:::\n:::\n\n\n#### Regression tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner = lrn(\"regr.rpart\", keep_model = TRUE)\n```\n:::\n\n\n##### Train\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$train(task, row_ids = train_set)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 209 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 209 45441.4500 43.27273  \n   2) ELOVL2_6< 56.675 98  5512.1220 30.24490  \n     4) ELOVL2_6< 47.24 47   866.4255 24.23404  \n       8) GRM2_9< 31.3 34   289.0588 22.29412 *\n       9) GRM2_9>=31.3 13   114.7692 29.30769 *\n     5) ELOVL2_6>=47.24 51  1382.6270 35.78431  \n      10) F5_2>=39.295 35   473.1429 33.28571 *\n      11) F5_2< 39.295 16   213.0000 41.25000 *\n   3) ELOVL2_6>=56.675 111  8611.3690 54.77477  \n     6) ELOVL2_6< 65.365 63  3101.2700 49.41270  \n      12) KLF14_2< 3.415 37  1059.0270 46.16216 *\n      13) KLF14_2>=3.415 26  1094.9620 54.03846 *\n     7) ELOVL2_6>=65.365 48  1321.3120 61.81250 *\n```\n\n\n:::\n:::\n\n\nWhat is odd about using a regression tree here is that we end up with only a few\ndiscrete estimates of age. Each \"leaf\" has a value. \n\n##### Predict\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train = learner$predict(task, row_ids=train_set)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test = learner$predict(task, row_ids=test_set)\n```\n:::\n\n\n##### Assess\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionRegr> for 209 observations:\n row_ids truth response\n     298    29 33.28571\n     103    58 61.81250\n     194    53 46.16216\n     ---   ---      ---\n     312    48 54.03846\n     246    66 61.81250\n     238    38 41.25000\n```\n\n\n:::\n:::\n\n\nWe can see the effect of the discrete values much more clearly here. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](machine_learning_mlr3_files/figure-pdf/unnamed-chunk-52-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAnd the r-squared values for this model prediction shows quite a bit of difference\nfrom the linear regression above. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq \n0.8995351 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionRegr> for 103 observations:\n row_ids truth response\n       4    37 41.25000\n       5    24 33.28571\n       7    34 33.28571\n     ---   ---      ---\n     306    42 46.16216\n     307    63 61.81250\n     309    68 61.81250\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq \n0.8545402 \n```\n\n\n:::\n:::\n\n\n#### RandomForest\n\nRandomforest is also tree-based, but unlike the single regression tree above,\nrandomforest is a \"forest\" of trees which will eliminate the discrete nature\nof a single tree. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner = lrn(\"regr.ranger\", mtry=2, min.node.size=20)\n```\n:::\n\n\n##### Train\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$train(task, row_ids = train_set)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      min.node.size = 20L, mtry = 2L, num.threads = 1L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      209 \nNumber of independent variables:  13 \nMtry:                             2 \nTarget node size:                 20 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       18.85364 \nR squared (OOB):                  0.9137009 \n```\n\n\n:::\n:::\n\n\n##### Predict\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train = learner$predict(task, row_ids=train_set)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test = learner$predict(task, row_ids=test_set)\n```\n:::\n\n\n##### Assess\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionRegr> for 209 observations:\n row_ids truth response\n     298    29 30.62154\n     103    58 58.05445\n     194    53 48.25661\n     ---   ---      ---\n     312    48 51.49846\n     246    66 64.39315\n     238    38 38.18038\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](machine_learning_mlr3_files/figure-pdf/unnamed-chunk-61-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     rsq \n0.960961 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionRegr> for 103 observations:\n row_ids truth response\n       4    37 37.79631\n       5    24 29.18371\n       7    34 33.26780\n     ---   ---      ---\n     306    42 40.29101\n     307    63 58.26534\n     309    68 63.15481\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq \n0.9208394 \n```\n\n\n:::\n:::\n\n\n## Example: Expression prediction from histone modification data\n\nIn this little set of exercises, you will be using histone marks near a gene to predict its expression (@fig-gene-expression).\n\n$$\ny ~ h1 + h2 + h3 + ...\n$$ {#eq-1}\n\n![What is the combined effect of histone marks on gene expression?](images/gene_expression_prediction_question.png){#fig-gene-expression}\n\nThe data are from a study that aimed to predict gene expression from histone modification data. The data include gene expression levels and histone modification data for a set of genes. The goal is to build a machine learning model that can predict gene expression levels based on the histone modification data. The histone modification data are simply summaries of the histone marks within the promoter, defined as the region 2kb upstream of the transcription start site for this exercise.\n\nWe will try a couple of different approaches:\n\n1. Penalized regression\n2. RandomForest\n\n### The Data\n\nThe data in this exercise were developed by Anshul Kundaje. We are not going to focus on the details of the data collection, etc. Instead, this is \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfullFeatureSet <- read.table(\"http://seandavi.github.io/ITR/expression-prediction/features.txt\");\n```\n:::\n\n\nWhat are the column names of the predictor variables?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(fullFeatureSet)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Control\"  \"Dnase\"    \"H2az\"     \"H3k27ac\"  \"H3k27me3\" \"H3k36me3\"\n [7] \"H3k4me1\"  \"H3k4me2\"  \"H3k4me3\"  \"H3k79me2\" \"H3k9ac\"   \"H3k9me1\" \n[13] \"H3k9me3\"  \"H4k20me1\"\n```\n\n\n:::\n:::\n\n\nThese are going to be predictors combined into a model. Some of our learners will rely on predictors\nbeing on a similar scale. Are our data already there?\n\nTo perform centering and scaling by column, we can convert to a matrix and then use `scale`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nscaled_features <- scale(as.matrix(fullFeatureSet))\nboxplot(fullFeatureSet, title='Original data')\nboxplot(scaled_features, title='Centered and scaled data')\n```\n\n::: {.cell-output-display}\n![Boxplots of original and scaled data.](machine_learning_mlr3_files/figure-pdf/fig-scaled-data-1.pdf){#fig-scaled-data fig-pos='H'}\n:::\n:::\n\n\nThere is a row for each gene and a column for each histone mark and we can see that the data are\ncentered and scaled by column. We can also see some patterns in the data (see @fig-heatmap). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsampled_features <- fullFeatureSet[sample(nrow(scaled_features), 500),]\nlibrary(ComplexHeatmap)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n========================================\nComplexHeatmap version 2.22.0\nBioconductor page: http://bioconductor.org/packages/ComplexHeatmap/\nGithub page: https://github.com/jokergoo/ComplexHeatmap\nDocumentation: http://jokergoo.github.io/ComplexHeatmap-reference\n\nIf you use it in published research, please cite either one:\n- Gu, Z. Complex Heatmap Visualization. iMeta 2022.\n- Gu, Z. Complex heatmaps reveal patterns and correlations in multidimensional \n    genomic data. Bioinformatics 2016.\n\n\nThe new InteractiveComplexHeatmap package can directly export static \ncomplex heatmaps into an interactive Shiny app with zero effort. Have a try!\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(ComplexHeatmap))\n========================================\n```\n\n\n:::\n\n```{.r .cell-code}\nHeatmap(sampled_features, name='histone marks', show_row_names=FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The input is a data frame-like object, convert it to a matrix.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Heatmap of 500 randomly sampled rows of the data. Columns are histone marks and there is a row for each gene.](machine_learning_mlr3_files/figure-pdf/fig-heatmap-1.pdf){#fig-heatmap fig-pos='H'}\n:::\n:::\n\n\nNow, we can read in the associated gene expression measures that will become\nour \"target\" for prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget <- scan(url(\"http://seandavi.github.io/ITR/expression-prediction/target.txt\"), skip=1)\n# make into a dataframe\nexp_pred_data <- data.frame(gene_expression=target, scaled_features)\n```\n:::\n\n\nAnd the first few rows of the target data frame using:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(exp_pred_data,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            gene_expression    Control      Dnase       H2az\nENSG00000000419.7.49575069         6.082343  0.7452926  0.7575546  1.0728432\nENSG00000000457.8.169863093        2.989145  1.9509786  1.0216546  0.3702787\nENSG00000000938.7.27961645        -5.058894 -0.3505542 -1.4482958 -1.0390775\n                               H3k27ac   H3k27me3   H3k36me3    H3k4me1\nENSG00000000419.7.49575069   1.0950440 -0.5125312  1.1334793  0.4127984\nENSG00000000457.8.169863093  0.7142157 -0.4079244  0.8739005  1.1649282\nENSG00000000938.7.27961645  -1.0173283  1.4117293 -0.5157582 -0.5017450\n                               H3k4me2    H3k4me3   H3k79me2     H3k9ac\nENSG00000000419.7.49575069   1.2136176  1.1202901  1.5155803  1.2468256\nENSG00000000457.8.169863093  0.6456572  0.6508561  0.7976487  0.5792891\nENSG00000000938.7.27961645  -0.1878255 -0.6560973 -1.3803974 -1.0067972\n                              H3k9me1   H3k9me3   H4k20me1\nENSG00000000419.7.49575069  0.1426980  1.185622  1.9599992\nENSG00000000457.8.169863093 0.3630902  1.014923 -0.2695111\nENSG00000000938.7.27961645  0.6564520 -1.370871 -1.8773178\n```\n\n\n:::\n:::\n\n\n### Create task\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp_pred_task = as_task_regr(exp_pred_data, target='gene_expression')\n```\n:::\n\n\nPartition the data into test and training sets. We will use $\\frac{1}{3}$ and $\\frac{2}{3}$ of the data for testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit = partition(exp_pred_task)\n```\n:::\n\n\n### Example learners\n\n#### Linear regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner = lrn(\"regr.lm\")\n```\n:::\n\n\n##### Train\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$train(exp_pred_task, split$train)\n```\n:::\n\n\n##### Predict\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train = learner$predict(exp_pred_task, split$train)\npred_test = learner$predict(exp_pred_task, split$test)\n```\n:::\n\n\n##### Assess\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionRegr> for 5789 observations:\n row_ids     truth  response\n       1  6.082343  5.182373\n       2  2.989145  2.970278\n       3 -5.058894 -5.283509\n     ---       ---       ---\n    8637 -5.058894 -3.955237\n    8638  6.089159  4.809801\n    8640 -3.114148 -4.723061\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(pred_train)\n```\n\n::: {.cell-output-display}\n![](machine_learning_mlr3_files/figure-pdf/unnamed-chunk-73-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nFor the training data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq \n0.7505366 \n```\n\n\n:::\n:::\n\n\nAnd the test data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq \n0.7510334 \n```\n\n\n:::\n:::\n\n\nAnd the plot of the test data predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pred_test)\n```\n\n::: {.cell-output-display}\n![](machine_learning_mlr3_files/figure-pdf/unnamed-chunk-76-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n#### Penalized regression\n\n\nImagine you want to teach a computer to predict house prices based on various features like size, number of bedrooms, and location. You decide to use **regression**, which finds a relationship between these features and the house prices. But what if your model becomes too complicated? This is where **penalized regression** comes in.\n\n\n##### The Problem with Overfitting\n\nSometimes, the model tries too hard to fit every single data point perfectly. This can make the model very complex, like trying to draw a perfect line through a very bumpy path. This problem is called **overfitting**. An overfitted model works well on the data it has seen (training data) but performs poorly on new, unseen data (testing data).\n\n##### Introducing Penalized Regression\n\nPenalized regression helps prevent overfitting by adding a \"penalty\" to the model for being too complex. Think of it as a way to encourage the model to be simpler and more general. There are three common types of penalized regression:\n\n1. **Ridge Regression (L2 Penalty):**\n   - Adds a penalty based on the size of the coefficients. It tries to keep all coefficients small.\n   - If the model’s equation looks too complicated, Ridge Regression will push it towards a simpler form by shrinking the coefficients.\n   - Imagine you have a rubber band that pulls the coefficients towards zero, making the model less likely to overfit.\n\n2. **Lasso Regression (L1 Penalty):**\n   - Adds a penalty that can shrink some coefficients all the way to zero.\n   - This means Lasso Regression can completely remove some features from the model, making it simpler.\n   - Imagine you have a pair of scissors that can cut off the least important features, leaving only the most important ones.\n\n3. **Elastic Net:**\n   - Combines both Ridge and Lasso penalties. It adds penalties for both the size and the number of coefficients.\n   - This method balances between shrinking coefficients and eliminating some altogether, offering the benefits of both Ridge and Lasso.\n   - Think of Elastic Net as using both the rubber band (Ridge) and scissors (Lasso) to simplify the model.\n\nWith our data, the number of predictors is not huge, but we might be interested in 1) reducing overfitting, 2) improving interpretability, or 3) both by minimizing the number of predictors in our model without drastically affecting our prediction accuracy. Without penalized regression, the model might come up with a very complex equation. With Ridge, Lasso, or Elastic Net, the model simplifies this equation by either shrinking the coefficients (Ridge), removing some of them (Lasso), or balancing both (Elastic Net).\n\nHere’s a simple summary:\n\n- **Ridge Regression:** Reduces the impact of less important features by shrinking their coefficients.\n- **Lasso Regression:** Can eliminate some features entirely by setting their coefficients to zero.\n- **Elastic Net:** Combines the effects of Ridge and Lasso, shrinking some coefficients and eliminating others.\n\nUsing penalized regression in machine learning ensures that your model:\n\n1. **Performs Better on New Data:** By avoiding overfitting, the model can make more accurate predictions on new, unseen data.\n2. **Is Easier to Interpret:** A simpler model with fewer features is easier to understand and explain.\n\n#### Penalized Regression with mlr3\n\nIn the mlr3 package, you can easily apply penalized regression methods to your tasks. Here’s how:\n\n1. **Select Penalized Regression Learners:** mlr3 provides learners for Ridge, Lasso, and Elastic Net Regression.\n2. **Train the Learner:** Use your data to train the chosen penalized regression model.\n3. **Evaluate and Adjust:** Check how well the model performs and make adjustments if needed.\n\n\nThis description explains penalized regression, including Ridge, Lasso, and Elastic Net, in an intuitive way, highlighting their benefits and how they work, while relating them to familiar concepts and the mlr3 package.\n\nRecall that we can use penalized regression to select the most important\npredictors from a large set of predictors. In this case, we will use the\n`glmnet` package to perform penalized regression, but we will use the\n`mlr` interface to `glmnet` to make it easier to use.\n\n\n\nThe `nfolds` parameter is the number of folds to use in the cross-validation\nprocedure. \n\nWhat is Cross-Validation? Cross-validation is a technique used to assess how well a model will perform on unseen data. It involves splitting the data into multiple parts, training the model on some of these parts, and validating it on the remaining parts. This process is repeated several times to ensure the model’s performance is consistent.\n\nWhy Use Cross-Validation?\nCross-validation helps to:\n\n* Avoid Overfitting: By testing the model on different subsets of the data, cross-validation helps ensure that the model does not memorize the training data but learns to generalize from it.\n* Select the Best Model Parameters: Penalized regression models, such as those trained with glmnet, have parameters that control the strength of the penalty (e.g., lambda). Cross-validation helps find the best values for these parameters.\n\nWhen using the glmnet package, cross-validation can be performed using the cv.glmnet function. Here’s how the process works:\n\n1. Split the Data:\n  The data is divided into \n  𝑘\n  k folds (common choices are 5 or 10 folds). Each fold is a subset of the data.\n\n2. Train and Validate:\n  The model is trained \n  𝑘\n  k times. In each iteration, \n  𝑘\n  −\n  1\n  k−1 folds are used for training, and the remaining fold is used for validation.\n  This process is repeated until each fold has been used as the validation set exactly once.\n\n3. Calculate Performance:\n  The performance of the model (e.g., mean squared error for regression) is calculated for each fold.\n  The average performance across all folds is computed to get an overall measure of how well the model is expected to perform on unseen data.\n\n4. Select the Best Parameters:\n  The cv.glmnet function evaluates different values of the penalty parameter (lambda).\n  It selects the lambda value that results in the best average performance across the folds.\n\nIn this case, we will use the `cv_glmnet` learner, which will automatically\nselect the best value of the penalization parameters. When the `alpha` parameter\nis set to 0, the model is a Ridge regression model. When the `alpha` parameter\nis set to 1, the model is a Lasso regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner = lrn(\"regr.cv_glmnet\", nfolds=10, alpha=0)\n```\n:::\n\n\n##### Train\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner$train(exp_pred_task)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('regr.rsq', 'regr.mse', 'regr.rmse'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq  regr.mse regr.rmse \n0.7505366 4.8335493 2.1985334 \n```\n\n\n:::\n:::\n\n\nIn the case of the penalized regression, we can also look at the coefficients\nof the model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(learner$model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept)  0.10173828\nControl     -0.07049573\nDnase        0.87495827\nH2az         0.33778155\nH3k27ac      0.19454755\nH3k27me3    -0.26196627\nH3k36me3     0.70067015\nH3k4me1     -0.06753216\nH3k4me2      0.15796965\nH3k4me3      0.38806405\nH3k79me2     0.94399114\nH3k9ac       0.50694074\nH3k9me1     -0.07422291\nH3k9me3     -0.17013775\nH4k20me1     0.11617186\n```\n\n\n:::\n:::\n\n\nNote that the coefficients are all zero for the histone marks that were not\nselected by the model. In this case, we can use the model not to predict new\ndata, but to help us understand the data. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train = learner$predict(exp_pred_task, split$train)\npred_test = learner$predict(exp_pred_task, split$test)\n```\n:::\n\n\n##### Assess\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionRegr> for 5789 observations:\n row_ids     truth  response\n       1  6.082343  4.892642\n       2  2.989145  2.932909\n       3 -5.058894 -4.518292\n     ---       ---       ---\n    8637 -5.058894 -4.341742\n    8638  6.089159  4.696146\n    8640 -3.114148 -4.153795\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(pred_train)\n```\n\n::: {.cell-output-display}\n![](machine_learning_mlr3_files/figure-pdf/unnamed-chunk-82-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nFor the training data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     rsq \n0.742853 \n```\n\n\n:::\n:::\n\n\nAnd the test data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rsq \n0.7428019 \n```\n\n\n:::\n:::\n\n\nAnd the plot of the test data predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pred_test)\n```\n\n::: {.cell-output-display}\n![](machine_learning_mlr3_files/figure-pdf/unnamed-chunk-85-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the R-squared value\ntruth <- pred_test$truth\npredicted <- pred_test$response\nrss <- sum((truth - predicted)^2)  # Residual sum of squares\ntss <- sum((truth - mean(truth))^2)  # Total sum of squares\nr_squared <- 1 - (rss / tss)\n```\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}