{
  "hash": "72bc799365204c8e5f7b6ad2da126d68",
  "result": {
    "engine": "knitr",
    "markdown": "# K-means clustering\n\n## History of the k-means algorithm\n \nThe k-means clustering algorithm was first proposed by Stuart Lloyd in 1957 as\na technique for pulse-code modulation. However, it was not published until\n1982. In 1965, Edward W. Forgy published an essentially identical method, which\nbecame widely known as the k-means algorithm. Since then, k-means clustering\nhas become one of the most popular unsupervised learning techniques in data\nanalysis and machine learning. \n \nK-means clustering is a method for finding patterns or groups in a dataset. It\nis an unsupervised learning technique, meaning that it doesn't rely on\npreviously labeled data for training. Instead, it identifies structures or\npatterns directly from the data based on the similarity between data points (see [@fig-kmeans-algorithm]).\n\n![K-means clustering takes a dataset and divides it into k clusters.](images/kmeans.png){#fig-kmeans-algorithm}\n\nIn simple terms, k-means clustering aims to divide a dataset into k distinct\ngroups or clusters, where each data point belongs to the cluster with the\nnearest mean (average). The goal is to minimize the variability within each\ncluster while maximizing the differences between clusters. This helps to reveal\nhidden patterns or relationships in the data that might not be apparent\notherwise.\n\n## The k-means algorithm\n \nThe k-means algorithm follows these general steps:\n\n1. Choose the number of clusters k.\n2. Initialize the cluster centroids randomly by selecting k data points from\n   the dataset.\n3. Assign each data point to the nearest centroid.\n4. Update the centroids by computing the mean of all the data points assigned\n   to each centroid.\n5. Repeat steps 3 and 4 until the centroids no longer change or a certain\n   stopping criterion is met (e.g., a maximum number of iterations).\n\nThe algorithm converges when the centroids stabilize or no longer change\nsignificantly. The final clusters represent the underlying patterns or\nstructures in the data. Advantages and disadvantages of k-means clustering\n \n## Pros and cons of k-means clustering\n\nCompared to other clustering algorithms, k-means has several advantages:\n\n* Simplicity and ease of implementation \n  : The k-means algorithm is relatively\n  straightforward and can be easily implemented, even for large datasets.\n* Scalability\n  : The algorithm can be adapted for large datasets using various optimization\n  techniques or parallel processing.\n* Speed\n  : K-means is generally faster than other clustering algorithms, especially\n  when the number of clusters k is small.\n* Interpretability\n  : The results of k-means clustering are easy to understand, as the algorithm\n  assigns each data point to a specific cluster based on its similarity to the\n  cluster's centroid.\n\nHowever, k-means clustering has several disadvantages as well:\n\n* Choice of k\n  : Selecting the appropriate number of clusters can be challenging\n  and often requires domain knowledge or experimentation. A poor choice of k\n  may yield poor results.\n* Sensitivity to initial conditions\n  : The algorithm's results can vary depending on the initial placement of\n  centroids. To overcome this issue, the algorithm can be run multiple times\n  with different initializations and the best solution can be chosen based on a\n  criterion (e.g., minimizing within-cluster variation).\n* Assumes spherical clusters\n  : K-means assumes that clusters are spherical and evenly sized, which may not\n  always be the case in real-world datasets. This can lead to poor performance\n  if the underlying clusters have different shapes or densities.\n* Sensitivity to outliers\n  : The algorithm is sensitive to outliers, which can heavily influence the\n  position of centroids and the final clustering result. Preprocessing the data\n  to remove or mitigate the impact of outliers can help improve the performance\n  of k-means clustering.\n\nDespite limitations, k-means clustering remains a popular and widely used\nmethod for exploring and analyzing data, particularly in biological data\nanalysis, where identifying patterns and relationships can provide valuable\ninsights into complex systems and processes.\n\n\n\n## An example of k-means clustering\n\n### The data and experimental background\n\nThe data we are going to use are from @derisi_exploring_1997. From \ntheir abstract:\n\n> DNA microarrays containing virtually every gene of Saccharomyces cerevisiae were used to carry out a comprehensive investigation of the temporal program of gene expression accompanying the metabolic shift from fermentation to respiration. The expression profiles observed for genes with known metabolic functions pointed to features of the metabolic reprogramming that occur during the diauxic shift, and the expression patterns of many previously uncharacterized genes provided clues to their possible functions. \n\n\nThese data are available from NCBI GEO as [GSE28](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE28).\n\nIn the case of the baker's or brewer's yeast Saccharomyces cerevisiae growing on glucose with plenty of aeration, the diauxic growth pattern is commonly observed in batch culture. During the first growth phase, when there is plenty of glucose and oxygen available, the yeast cells prefer glucose fermentation to aerobic respiration even though aerobic respiration is the more efficient pathway to grow on glucose. This experiment \nprofiles gene expression for 6400 genes over a time course during which the cells are undergoing\na [diauxic shift](https://en.wikipedia.org/wiki/Diauxie).\n\nThe data in deRisi et al. have no replicates and are time course data. Sometimes, seeing how groups \nof genes behave can give biological insight into the experimental system or the function of individual\ngenes. We can use clustering to group genes that have a similar expression pattern over time and then\npotentially look at the genes that do so. \n\nOur goal, then, is to use `kmeans` clustering to divide highly variable (informative) genes into groups\nand then to visualize those groups.\n\n## Getting data\n\nThese data were deposited at NCBI GEO back in 2002. GEOquery can pull them out easily.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GEOquery)\ngse = getGEO(\"GSE28\")[[1]]\nclass(gse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ExpressionSet\"\nattr(,\"package\")\n[1] \"Biobase\"\n```\n\n\n:::\n:::\n\n\nGEOquery is a little dated and was written before the SummarizedExperiment existed. However, Bioconductor makes\na conversion from the old ExpressionSet that GEOquery uses to the SummarizedExperiment that we see\nso commonly used now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SummarizedExperiment)\ngse = as(gse, \"SummarizedExperiment\")\ngse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclass: SummarizedExperiment \ndim: 6400 7 \nmetadata(3): experimentData annotation protocolData\nassays(1): exprs\nrownames(6400): 1 2 ... 6399 6400\nrowData names(20): ID ORF ... FAILED IS_CONTAMINATED\ncolnames(7): GSM887 GSM888 ... GSM892 GSM893\ncolData names(33): title geo_accession ... supplementary_file\n  data_row_count\n```\n\n\n:::\n:::\n\n\nTaking a quick look at the `colData()`, it might be that we want to reorder the columns a bit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolData(gse)$title\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"diauxic shift timecourse: 15.5 hr\" \"diauxic shift timecourse: 0 hr\"   \n[3] \"diauxic shift timecourse: 18.5 hr\" \"diauxic shift timecourse: 9.5 hr\" \n[5] \"diauxic shift timecourse: 11.5 hr\" \"diauxic shift timecourse: 13.5 hr\"\n[7] \"diauxic shift timecourse: 20.5 hr\"\n```\n\n\n:::\n:::\n\n\nSo, we can reorder by hand to get the time course correct:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngse = gse[, c(2,4,5,6,1,3,7)]\n```\n:::\n\n\n## Preprocessing\n\nIn gene expression data analysis, the primary objective is often to identify\ngenes that exhibit significant differences in expression levels across various\nconditions, such as diseased vs. healthy samples or different time points in a\ntime-course experiment. However, gene expression datasets are typically large,\nnoisy, and contain numerous genes that do not exhibit substantial changes in\nexpression levels. Analyzing all genes in the dataset can be computationally\nintensive and may introduce noise or false positives in the results.\n\nOne common approach to reduce the complexity of the dataset and focus on the\nmost informative genes is to subset the genes based on their standard deviation\nin expression levels across the samples. The standard deviation is a measure of\ndispersion or variability in the data, and genes with high standard deviations\nhave more variation in their expression levels across the samples.\n\nBy selecting genes with high standard deviations, we focus on genes that show\nrelatively large changes in expression levels across different conditions.\nThese genes are more likely to be biologically relevant and involved in the\nunderlying processes or pathways of interest. In contrast, genes with low\nstandard deviations exhibit little or no change in expression levels and are\nless likely to be informative for the analysis. It turns out that applying\nfiltering based on criteria such as standard deviation can also increase power\nand reduce false positives in the analysis [@bourgon_independent_2010].\n\nTo subset the genes for analysis based on their standard deviation, the\nfollowing steps can be followed: Calculate the standard deviation of each\ngene's expression levels across all samples. Set a threshold for the standard\ndeviation, which can be determined based on domain knowledge, data\ndistribution, or a specific percentile of the standard deviation values (e.g.,\nselecting the top 10% or 25% of genes with the highest standard deviations).\nRetain only the genes with a standard deviation above the chosen threshold for\nfurther analysis.\n\nBy subsetting the genes based on their standard deviation, we can reduce the\ncomplexity of the dataset, speed up the subsequent analysis, and increase the\nlikelihood of detecting biologically meaningful patterns and relationships in\nthe gene expression data. The threshold for the standard deviation cutoff is\nrather arbitrary, so it may be beneficial to try a few to check for sensitivity\nof findings. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsds = apply(assays(gse)[[1]], 1, sd)\nhist(sds)\n```\n\n::: {.cell-output-display}\n![Histogram of standard deviations for all genes in the deRisi dataset.](kmeans_files/figure-html/fig-sds-derisi-1.png){#fig-sds-derisi width=672}\n:::\n:::\n\n\nExamining the plot, we can see that the most highly variable genes have an sd >\n0.8 or so (arbitrary). We can, for convenience, create a new\n`SummarizedExperiment` that contains only our most highly variable genes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nidx = sds>0.8 & !is.na(sds)\ngse_sub = gse[idx,]\n```\n:::\n\n\n## Clustering\n\nNow, `gse_sub` contains a subset of our data. \n\nThe `kmeans` function takes a matrix and the number\nof clusters as arguments. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nk = 4\nkm = kmeans(assays(gse_sub)[[1]], 4)\n```\n:::\n\n\nThe `km` kmeans result contains a vector, `km$cluster`, which gives the cluster\nassociated with each gene. We can plot the genes for each cluster to see how\nthese different genes behave.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpression_values = assays(gse_sub)[[1]]\npar(mfrow=c(2,2), mar=c(3,4,1,2)) # this allows multiple plots per page\nfor(i in 1:k) {\n    matplot(t(expression_values[km$cluster==i, ]), type='l', ylim=c(-3,3),\n            ylab = paste(\"cluster\", i))\n}\n```\n\n::: {.cell-output-display}\n![Gene expression profiles for the four clusters identified by k-means clustering. Each line represents a gene in the cluster, and each column represents a time point in the experiment. Each cluster shows a distinct trend where the genes in the cluster are potentially co-regulated. ](kmeans_files/figure-html/fig-kmeans-derisi-1.png){#fig-kmeans-derisi width=672}\n:::\n:::\n\n\n\nTry this with different size k. Perhaps go back to choose more genes (using a\nsmaller cutoff for sd).\n\n## Summary\n\nIn this lesson, we have learned how to use k-means clustering to identify \ngroups of genes that behave similarly over time. We have also learned how to\nsubset our data to focus on the most informative genes.\n\n\n\n",
    "supporting": [
      "kmeans_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}