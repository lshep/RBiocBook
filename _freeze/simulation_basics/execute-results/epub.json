{
  "hash": "5ca9eb7168a5872668ac964a994e235e",
  "result": {
    "engine": "knitr",
    "markdown": "---\nauthor: \"Garrett Grolemund\"\n---\n\n# Simulation Basics\n\nNow that we can roll dice and have our tools loaded, let's explore one of the most powerful techniques in data science: **simulation**. Simulation allows us to understand complex systems by running many trials and observing the patterns that emerge.\n\nOur dice rolling function gives us a perfect starting point to learn simulation concepts. We'll use it to answer questions like \"Are our dice really fair?\" and \"What happens when we roll dice thousands of times?\"\n\n## The Power of Repetition\n\nLet's start with our dice rolling function from earlier:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroll2 <- function(bones = 1:6) {\n  dice = sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n```\n:::\n\n\nRolling once gives us a single result, but that doesn't tell us much about the overall behavior of our dice. What we really want to know is: **What pattern emerges when we roll many times?**\n\n## Meet `replicate()`\n\nR provides a built-in function called `replicate()` that makes it easy to repeat any operation many times. Let's look at its help page to understand how it works:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhelp(\"replicate\")\n```\n:::\n\n\nThe `replicate()` function takes two main arguments:\n- `n`: the number of times to repeat the operation\n- `expr`: the expression (code) to repeat\n\nLet's try it with our dice:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Roll our dice 10 times\nrolls_10 <- replicate(n = 10, roll2())\nrolls_10\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 3 9 7 8 9 8 5 8 8 9\n```\n\n\n:::\n:::\n\n\nEach time you run this code, you'll get different results because we're sampling randomly. That's exactly what we want!\n\n## Understanding Our Results\n\nThe `replicate()` function returns a **vector** containing all the results. Let's explore what our 10 rolls tell us:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Basic information about our rolls\nlength(rolls_10)  # How many rolls?\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(rolls_10)    # Average result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.4\n```\n\n\n:::\n\n```{.r .cell-code}\nmin(rolls_10)     # Lowest sum\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(rolls_10)     # Highest sum\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9\n```\n\n\n:::\n:::\n\n\nBut 10 rolls isn't very many. What happens if we increase the number of trials?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Roll 100 times\nrolls_100 <- replicate(n = 100, roll2())\n\n# Look at the first few results\nhead(rolls_100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12  6  7  9  3  6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary statistics\nsummary(rolls_100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0     5.0     7.0     6.6     8.0    12.0 \n```\n\n\n:::\n:::\n\n\n## The Magic of Large Numbers\n\nAs we increase our sample size, something interesting happens. The average starts to converge toward the theoretical expectation. Let's see this in action:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare different sample sizes\nsample_sizes <- c(10, 100, 1000, 10000)\n\nfor(n in sample_sizes) {\n  rolls <- replicate(n, roll2())\n  avg <- mean(rolls)\n  cat(\"Sample size:\", n, \"- Average:\", round(avg, 2), \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample size: 10 - Average: 7.1 \nSample size: 100 - Average: 6.85 \nSample size: 1000 - Average: 7.07 \nSample size: 10000 - Average: 6.97 \n```\n\n\n:::\n:::\n\n\nNotice how the average gets closer to 7 (the theoretical expected value for two fair dice) as we increase the sample size? This demonstrates the **Law of Large Numbers**, one of the fundamental principles in statistics.\n\n## Checking if Our Dice Are Fair\n\nIf our dice are truly fair, we should see each possible sum with its expected frequency. For two six-sided dice, the possible sums range from 2 to 12, but they're not equally likely:\n\n- Sum of 7: Can occur 6 ways (1+6, 2+5, 3+4, 4+3, 5+2, 6+1)\n- Sum of 2: Can occur 1 way (1+1)\n- Sum of 12: Can occur 1 way (6+6)\n\nLet's generate a large sample and examine the distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate a large sample\nlarge_sample <- replicate(n = 10000, roll2())\n\n# Count how often each sum appears\ntable(large_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlarge_sample\n   2    3    4    5    6    7    8    9   10   11   12 \n 320  522  835 1143 1417 1660 1388 1062  838  552  263 \n```\n\n\n:::\n:::\n\n\nWe can also calculate proportions to see the relative frequencies:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert counts to proportions\nprop.table(table(large_sample))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlarge_sample\n     2      3      4      5      6      7      8      9     10     11     12 \n0.0320 0.0522 0.0835 0.1143 0.1417 0.1660 0.1388 0.1062 0.0838 0.0552 0.0263 \n```\n\n\n:::\n:::\n\n\n## Saving Our Results\n\nSince generating large samples can take time, it's often useful to save our results for later analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save our simulation results\nwrite.csv(\n  data.frame(roll_result = large_sample), \n  file = \"dice_simulation_10k.csv\", \n  row.names = FALSE\n)\n```\n:::\n\n\nWe can then read these results back later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read back our saved results\nsaved_rolls <- read.csv(\"dice_simulation_10k.csv\")\nhead(saved_rolls)\n```\n:::\n\n\n## Practical Applications\n\nThis simulation approach isn't just for dice games. You can use the same principles to:\n\n- Test statistical models\n- Estimate probabilities for complex scenarios  \n- Validate theoretical calculations\n- Generate synthetic data for testing\n\n## Key Takeaways\n\n1. **Simulation reveals patterns**: Single trials are unpredictable, but many trials show clear patterns\n2. **Sample size matters**: Larger samples give more reliable estimates  \n3. **`replicate()` is your friend**: It makes running many trials easy\n4. **Save your work**: Large simulations take time, so save results for later analysis\n\n## Exercise\n\nTry modifying the `roll2()` function to create weighted dice (hint: look at the `prob` argument in the `sample()` function). Then use simulation to verify that your weighting works as expected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your weighted dice function here\nroll2_weighted <- function(bones = 1:6) {\n  # Add weighting logic\n}\n\n# Test with simulation\nweighted_results <- replicate(n = 1000, roll2_weighted())\ntable(weighted_results)\n```\n:::\n\n\nIn our next chapter, we'll learn how to visualize these simulation results to make the patterns even clearer!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}