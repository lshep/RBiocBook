{
  "hash": "3f960cebf46803f89f8f343732a6026f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supervised Learning\"\n---\n\nIn the previous chapter, we introduced the three main paradigms of machine learning: supervised, unsupervised, and reinforcement learning. We also highlighted the importance of a structured workflow, particularly the critical step of data splitting to prevent overfitting. Now, we'll dive deeper into the world of **supervised learning**, the most common and widely applied category of machine learning.\n\nThis chapter provides a practical framework for understanding how different supervised learning algorithms work. Instead of viewing each algorithm as a completely separate entity, we'll see that they all share a common conceptual structure. By grasping this structure, you can more easily understand, compare, and select the right algorithm for your problem. We will then use this framework to explore some of the most fundamental and popular algorithms for both regression and classification tasks.\n\n## A Framework for Understanding Supervised Algorithms\n\nAt a high level, every supervised learning algorithm can be understood as having three core components: the **hypothesis space**, the **objective function**, and the **learning algorithm**. Thinking about new algorithms in terms of these three parts can demystify how they operate and highlight their key differences.\n\n::: {.callout-note}\n### The Three Pillars of a Supervised Algorithm\n\n1.  **Hypothesis Space (The Model Family):** This is the set of all possible models the algorithm is allowed to consider. For example, in linear regression, the hypothesis space is the set of all possible straight lines. For a decision tree, it's the set of all possible branching rules. The choice of hypothesis space imposes a certain structure or *bias* on the solution.\n\n2.  **Objective Function (The Goal):** This function defines what a \"good\" model looks like. It measures how well a particular model from the hypothesis space fits the training data. For instance, in regression, a common objective function is the *Mean Squared Error (MSE)*, which penalizes models for making large prediction errors. The goal of learning is to find the model that minimizes this objective function.\n\n3.  **Learning Algorithm (The Optimizer):** This is the computational procedure used to search through the hypothesis space and find the model that best minimizes the objective function. This could be a straightforward mathematical formula (as in simple linear regression) or a complex iterative process (as in training neural networks).\n:::\n\nBy framing algorithms in this way, we can see, for example, that Ridge and Lasso regression share the same hypothesis space and learning algorithm as linear regression but differ in their objective function, which includes a penalty for complexity. This small change in the objective function has profound effects on the final model, helping to prevent overfitting.\n\n## Supervised Learning\n\n### Linear regression\n\nIn [statistics](https://en.wikipedia.org/wiki/Statistics \"Statistics\"), **linear regression** is a [linear](https://en.wikipedia.org/wiki/Linearity \"Linearity\") approach for modelling the relationship between a [scalar](https://en.wikipedia.org/wiki/Scalar_(mathematics) \"Scalar (mathematics)\") response and one or more explanatory variables (also known as [dependent and independent variables](https://en.wikipedia.org/wiki/Dependent_and_independent_variables \"Dependent and independent variables\")). The case of one explanatory variable is called _[simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression \"Simple linear regression\")_; for more than one, the process is called **multiple linear regression**. This term is distinct from [multivariate linear regression](https://en.wikipedia.org/wiki/Multivariate_linear_regression \"Multivariate linear regression\"), where multiple [correlated](https://en.wikipedia.org/wiki/Correlation_and_dependence \"Correlation and dependence\") dependent variables are predicted, rather than a single scalar variable.\n\nIn linear regression, the relationships are modeled using [linear predictor functions](https://en.wikipedia.org/wiki/Linear_predictor_function \"Linear predictor function\") whose unknown model [parameters](https://en.wikipedia.org/wiki/Parameters \"Parameters\") are [estimated](https://en.wikipedia.org/wiki/Estimation_theory \"Estimation theory\") from the [data](https://en.wikipedia.org/wiki/Data \"Data\"). Such models are called [linear models](https://en.wikipedia.org/wiki/Linear_model \"Linear model\"). Most commonly, the [conditional mean](https://en.wikipedia.org/wiki/Conditional_expectation \"Conditional expectation\") of the response given the values of the explanatory variables (or predictors) is assumed to be an [affine function](https://en.wikipedia.org/wiki/Affine_transformation \"Affine transformation\") of those values; less commonly, the conditional [median](https://en.wikipedia.org/wiki/Median \"Median\") or some other [quantile](https://en.wikipedia.org/wiki/Quantile \"Quantile\") is used. Like all forms of [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis \"Regression analysis\"), linear regression focuses on the [conditional probability distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution \"Conditional probability distribution\") of the response given the values of the predictors, rather than on the [joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution \"Joint probability distribution\") of all of these variables, which is the domain of [multivariate analysis](https://en.wikipedia.org/wiki/Multivariate_analysis \"Multivariate analysis\").\n\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\n\n### K-nearest Neighbor\n\n\n::: {.cell}\n::: {.cell-output-display}\n![**Figure**. The k-nearest neighbor algorithm can be used for regression or classification. ](https://www.kdnuggets.com/wp-content/uploads/rapidminer-knn-image1.jpg)\n:::\n:::\n\n\nThe **_k_\\-nearest neighbors algorithm** (**_k_\\-NN**) is a [non-parametric](https://en.wikipedia.org/wiki/Non-parametric_statistics \"Non-parametric statistics\") [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning \"Supervised learning\") method first developed by [Evelyn Fix](https://en.wikipedia.org/wiki/Evelyn_Fix \"Evelyn Fix\") and [Joseph Hodges](https://en.wikipedia.org/wiki/Joseph_Lawson_Hodges_Jr. \"Joseph Lawson Hodges Jr.\") in 1951, and later expanded by [Thomas Cover](https://en.wikipedia.org/wiki/Thomas_M._Cover \"Thomas M. Cover\"). It is used for [classification](https://en.wikipedia.org/wiki/Statistical_classification \"Statistical classification\") and [regression](https://en.wikipedia.org/wiki/Regression_analysis \"Regression analysis\"). In both cases, the input consists of the _k_ closest training examples in a [data set](https://en.wikipedia.org/wiki/Data_set \"Data set\"). \n\nThe k-nearest neighbor (k-NN) algorithm is a simple, yet powerful, supervised\nmachine learning method used for classification and regression tasks. It is an\ninstance-based, non-parametric learning method that stores the entire training\ndataset and makes predictions based on the similarity between data points. The\nunderlying principle of the k-NN algorithm is that similar data points (those\nthat are close to each other in multidimensional space) are likely to have\nsimilar outcomes or belong to the same class.\n\nHere's a description of how the k-NN algorithm works:\n\n1. Determine the value of k: The first step is to choose the number of nearest\n   neighbors (k) to consider when making predictions. The value of k is a\n   user-defined hyperparameter and can significantly impact the algorithm's\n   performance. A small value of k can lead to overfitting, while a large value\n   may result in underfitting.\n1. Compute distance: Calculate the distance between the new data point (query\n   point) and each data point in the training dataset. The most common distance\n   metrics used are Euclidean, Manhattan, and Minkowski distance. The choice of\n   distance metric depends on the problem and the nature of the data.\n1. Find k-nearest neighbors: Identify the k data points in the training dataset\n   that are closest to the query point, based on the chosen distance metric.\n1. Make predictions: Once the k-nearest neighbors are identified, the final\n   step is to make predictions. The prediction for the query point can be made\n   in two ways:\n    a. For classification, determine the class labels of the k-nearest\n    neighbors and assign the class label with the highest frequency (majority\n    vote) to the query point. In case of a tie, one can choose the class with\n    the smallest average distance to the query point or randomly select one\n    among the tied classes.\n    b. For regression tasks, the k-NN algorithm follows a similar process, but\n    instead of majority voting, it calculates the mean (or median) of the\n    target values of the k-nearest neighbors and assigns it as the prediction\n    for the query point.\n\nThe k-NN algorithm is known for its simplicity, ease of implementation, and\nability to handle multi-class problems. However, it has some drawbacks, such as\nhigh computational cost (especially for large datasets), sensitivity to the\nchoice of k and distance metric, and poor performance with high-dimensional or\nnoisy data. Scaling and preprocessing the data, as well as using dimensionality\nreduction techniques, can help mitigate some of these issues.\n\n\n\n*  In _k-NN classification_, the output is a class membership. An object is\n  classified by a plurality vote of its neighbors, with the object being\n  assigned to the class most common among its _k_ nearest neighbors (_k_ is a\n  positive [integer](https://en.wikipedia.org/wiki/Integer \"Integer\"),\n  typically small). If _k_ = 1, then the object is simply assigned to the class\n  of that single nearest neighbor.\n\n*  In _k-NN regression_, the output is the property value for the object. This\n  value is the average of the values of _k_ nearest neighbors.\n\n_k_\\-NN is a type of\n[classification](https://en.wikipedia.org/wiki/Classification \"Classification\")\nwhere the function is only approximated locally and all computation is deferred\nuntil function evaluation. Since this algorithm relies on distance for\nclassification, if the features represent different physical units or come in\nvastly different scales then\n[normalizing](https://en.wikipedia.org/wiki/Normalization_(statistics)\n\"Normalization (statistics)\") the training data can improve its accuracy\ndramatically.\n\nBoth for classification and regression, a useful technique can be to assign\nweights to the contributions of the neighbors, so that the nearer neighbors\ncontribute more to the average than the more distant ones. For example, a\ncommon weighting scheme consists in giving each neighbor a weight of 1/_d_,\nwhere _d_ is the distance to the neighbor.\n\nThe neighbors are taken from a set of objects for which the class (for _k_\\-NN\nclassification) or the object property value (for _k_\\-NN regression) is known.\nThis can be thought of as the training set for the algorithm, though no\nexplicit training step is required.\n\n## Penalized regression\n\nAdapted from http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/.\n\nPenalized regression is a type of regression analysis that introduces a penalty term to the loss function in order to prevent overfitting and improve the model's ability to generalize. Remember that in regression, the *loss* function is the sum of squares @eq-sum-of-squares.\n\n$$L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2$$ {#eq-sum-of-squares}\n\nIn @eq-sum-of-squares, $\\hat{y}_i$ is the predicted output, $y_i$ is the actual output, and n is the number of observations. The goal of regression is to minimize the loss function by finding the optimal values of the model parameters or coefficients. The model parameters are estimated using the training data. The model is then evaluated using the test data. If the model performs well on the training data but poorly on the test data, it is said to be overfit. Overfitting occurs when the model learns the training data too well, including the noise, and is not able to generalize well to new data. This is a common problem in machine learning, particularly when there are a large number of predictors compared to the number of observations, and can be addressed by penalized regression.\n\n\nThe two most common types of penalized regression are Ridge Regression (L2 penalty) and LASSO Regression (L1 penalty). Both Ridge and LASSO help to reduce model complexity and prevent over-fitting which may result from simple linear regression. However, the choice between Ridge and LASSO depends on the situation and the dataset at hand. If feature selection is important for the interpretation of the model, LASSO might be preferred. If the goal is prediction accuracy and the model needs to retain all features, Ridge might be the better choice.\n\n### Ridge regression\n\nRidge regression shrinks the regression coefficients, so that variables, with\nminor contribution to the outcome, have their coefficients close to zero. The\nshrinkage of the coefficients is achieved by penalizing the regression model\nwith a penalty term called L2-norm, which is the sum of the squared\ncoefficients. The amount of the penalty can be fine-tuned using a constant\ncalled lambda (λ). Selecting a good value for λ is critical. When λ=0, the\npenalty term has no effect, and ridge regression will produce the classical\nleast square coefficients. However, as λ increases to infinite, the impact of\nthe shrinkage penalty grows, and the ridge regression coefficients will get\nclose zero. The loss function for Ridge Regression is:\n\n$$L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 + \\lambda\\sum_{j=0}^{k} \\beta_j^2$$ {#eq-ridge} \n\nHere, $\\hat{y}_i$ is the predicted output, $y_i$ is the actual output,\n${\\beta_j}$ represents the model parameters or coefficients, and λ is the\nregularization parameter. The second term, λ∑βj^2, is the penalty term where\nall parameters are squared and summed. Ridge regression tends to shrink the\ncoefficients but doesn't necessarily zero them.\n\n\n\nNote that, in contrast to the ordinary least square regression, ridge\nregression is highly affected by the scale of the predictors. Therefore, it is\nbetter to standardize (i.e., scale) the predictors before applying the ridge\nregression (James et al. 2014), so that all the predictors are on the same\nscale. The standardization of a predictor x, can be achieved using the formula\n$x' = \\frac{x}{sd(x)}$, where $sd(x)$ is the standard deviation of $x$. The consequence of\nthis is that, all standardized predictors will have a standard deviation of one\nallowing the final fit to not depend on the scale on which the predictors are\nmeasured.\n\nOne important advantage of the ridge regression, is that it still performs\nwell, compared to the ordinary least square method (see @eq-sum-of-squares), \nin a situation where you have a large multivariate\ndata with the number of predictors (p) larger than the number of observations\n(n). One disadvantage of the ridge regression is that, it will include all the\npredictors in the final model, unlike the stepwise regression methods, which\nwill generally select models that involve a reduced set of variables. Ridge\nregression shrinks the coefficients towards zero, but it will not set any of\nthem exactly to zero. The LASSO regression is an alternative that overcomes\nthis drawback.\n\n### LASSO regression\n\nLASSO stands for _Least Absolute Shrinkage and Selection Operator_. It shrinks\nthe regression coefficients toward zero by penalizing the regression model with\na penalty term called L1-norm, which is the sum of the absolute coefficients.\nIn the case of LASSO regression, the penalty has the effect of forcing some of\nthe coefficient estimates, with a minor contribution to the model, to be\nexactly equal to zero. This means that, LASSO can be also seen as an\nalternative to the subset selection methods for performing variable selection\nin order to reduce the complexity of the model. As in ridge regression,\nselecting a good value of $\\lambda$ for the LASSO is critical. The loss function for\nLASSO Regression is:\n\n$$L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 + \\lambda\\sum_{j=0}^{k} |\\beta_j|$$ {#eq-lasso}\n\nSimilar to Ridge, ŷi is the predicted output, yi is the actual output, βj\nrepresents the model parameters or coefficients, and λ is the regularization\nparameter. The second term, λ∑|βj|, is the penalty term where the absolute\nvalues of all parameters are summed. LASSO regression tends to shrink the\ncoefficients and can zero out some of them, effectively performing variable\nselection.\n\n\nOne obvious advantage of LASSO regression over ridge regression, is that it\nproduces simpler and more interpretable models that incorporate only a reduced\nset of the predictors. However, neither ridge regression nor the LASSO will\nuniversally dominate the other. Generally, LASSO might perform better in a\nsituation where some of the predictors have large coefficients, and the\nremaining predictors have very small coefficients. Ridge regression will\nperform better when the outcome is a function of many predictors, all with\ncoefficients of roughly equal size (James et al. 2014).\n\nCross-validation methods can be used for identifying which of these two\ntechniques is better on a particular data set.\n\n### Elastic Net\n\nElastic Net produces a regression model that is penalized with both the L1-norm\nand L2-norm. The consequence of this is to effectively shrink coefficients\n(like in ridge regression) and to set some coefficients to zero (as in LASSO).\n\n\n\n\n### Classification and Regression Trees (CART)\n\n[Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning) is supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations. Decision trees are a popular machine learning method used for both classification and regression tasks. They are hierarchical, tree-like structures that model the relationship between features and the target variable by recursively splitting the data into subsets based on the feature values. Each internal node in the tree represents a decision or test on a feature, and each branch represents the outcome of that test. The leaf nodes contain the final prediction, which is the majority class for classification tasks or the mean/median of the target values for regression tasks.\n\n![Graphical representation of a decision tree.](https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg){#fig-decision-tree fig-align=\"center\"}\n\n\n\n\n\nHere's an overview of the decision tree learning process:\n\n*  Select the best feature and split value: Start at the root node and choose the feature and split value that results in the maximum reduction of impurity (or increase in information gain) in the child nodes. For classification tasks, impurity measures like Gini index or entropy are commonly used, while for regression tasks, mean squared error (MSE) or mean absolute error (MAE) can be used.\n*  Split the data: Partition the dataset into subsets based on the chosen feature and split value.\n*  Recursion: Repeat steps 1 and 2 for each subset until a stopping criterion is met. Stopping criteria can include reaching a maximum tree depth, a minimum number of samples per leaf, or no further improvement in impurity.\n*  Prune the tree (optional): To reduce overfitting, decision trees can be pruned by removing branches that do not significantly improve the model's performance on the validation dataset. This can be done using techniques like reduced error pruning or cost-complexity pruning.\n\nDecision trees have several advantages, such as:\n\n*  Interpretability\n  : They are easy to understand, visualize, and explain, even for non-experts.\n*  Minimal data preprocessing\n  : Decision trees can handle both numerical and categorical data, and they are robust to outliers and missing values.\n*  Non-linear relationships\n  : They can capture complex non-linear relationships between features and the target variable.\n\nHowever, decision trees also have some drawbacks:\n\n*  Overfitting\n  : They are prone to overfitting, especially when the tree is deep or has few samples per leaf. Pruning and setting stopping criteria can help mitigate this issue.\n*  Instability\n  : Small changes in the data can lead to different tree structures. This can be addressed by using ensemble methods like random forests or gradient boosting machines (GBMs).\n*  Greedy learning\n  : Decision tree algorithms use a greedy approach, meaning they make locally optimal choices at each node. This may not always result in a globally optimal tree.\n\nDespite these limitations, decision trees are widely used in various applications due to their simplicity, interpretability, and ability to handle diverse data types.\n\n### RandomForest\n\n**Random forests** or **random decision forests** is an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning \"Ensemble learning\") method for [classification](https://en.wikipedia.org/wiki/Statistical_classification \"Statistical classification\"), [regression](https://en.wikipedia.org/wiki/Regression_analysis \"Regression analysis\") and other tasks that operates by constructing a multitude of [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning \"Decision tree learning\") at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of [overfitting](https://en.wikipedia.org/wiki/Overfitting \"Overfitting\") to their [training set](https://en.wikipedia.org/wiki/Test_set \"Test set\"). Random forests generally outperform [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning \"Decision tree learning\"), but their accuracy is lower than gradient boosted trees\\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed \"Wikipedia:Citation needed\")_\\]. However, data characteristics can affect their performance.\n\nThe first algorithm for random decision forests was created in 1995 by [Tin Kam Ho](https://en.wikipedia.org/wiki/Tin_Kam_Ho \"Tin Kam Ho\") using the [random subspace method](https://en.wikipedia.org/wiki/Random_subspace_method \"Random subspace method\"), which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n\nAn extension of the algorithm was developed by [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman \"Leo Breiman\") and [Adele Cutler](https://en.wikipedia.org/wiki/Adele_Cutler \"Adele Cutler\"), who registered \"Random Forests\" as a [trademark](https://en.wikipedia.org/wiki/Trademark \"Trademark\") in 2006 (as of 2019[\\[update\\]](https://en.wikipedia.org/w/index.php?title=Random_forest&action=edit), owned by [Minitab, Inc.](https://en.wikipedia.org/wiki/Minitab \"Minitab\")). The extension combines Breiman's \"[bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating \"Bootstrap aggregating\")\" idea and random selection of features, introduced first by Ho and later independently by Amit and [Geman](https://en.wikipedia.org/wiki/Donald_Geman \"Donald Geman\") in order to construct a collection of decision trees with controlled variance.\n\nRandom forests are frequently used as \"blackbox\" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.\n\n\n![Graphical representation of random forests. ](https://miro.medium.com/v2/resize:fit:592/1*i0o8mjFfCn-uD79-F1Cqkw.png){#fig-random-forest fig-align=\"center\"}\n\n\n",
    "supporting": [
      "models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}